{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [First Bullet Header](#first-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from sklearn.metrics import accuracy_score,r2_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/final.csv' ,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ATP</th>\n",
       "      <th>B365L</th>\n",
       "      <th>B365W</th>\n",
       "      <th>Best of</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Court</th>\n",
       "      <th>Date</th>\n",
       "      <th>L1</th>\n",
       "      <th>...</th>\n",
       "      <th>one_three_vv</th>\n",
       "      <th>one_five_vv</th>\n",
       "      <th>pts_diff</th>\n",
       "      <th>player_one_pts</th>\n",
       "      <th>player_two_pts</th>\n",
       "      <th>player_one_total_games</th>\n",
       "      <th>player_two_total_games</th>\n",
       "      <th>total_sets</th>\n",
       "      <th>player_one_sets</th>\n",
       "      <th>player_two_sets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73f14b1f-a9af-4ef9-88a0-df3b4ff82e12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-192</td>\n",
       "      <td>1111</td>\n",
       "      <td>919</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77cb58d6-44ed-4875-a622-4c6839b08072</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-165</td>\n",
       "      <td>803</td>\n",
       "      <td>638</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9bee2c88-a8e5-4169-b0d6-5ad4a030d689</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158</td>\n",
       "      <td>1025</td>\n",
       "      <td>867</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7a40475-873d-426e-b37d-7f4c0dbd11c3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.83</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>224</td>\n",
       "      <td>905</td>\n",
       "      <td>681</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2b89806d-1688-4364-88c6-c1ffc499e63e</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284</td>\n",
       "      <td>920</td>\n",
       "      <td>636</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  Unnamed: 0  ATP  B365L  B365W  \\\n",
       "0  73f14b1f-a9af-4ef9-88a0-df3b4ff82e12           0    1   1.72   2.00   \n",
       "1  77cb58d6-44ed-4875-a622-4c6839b08072           1    1   2.25   1.57   \n",
       "2  9bee2c88-a8e5-4169-b0d6-5ad4a030d689           2    1   3.75   1.25   \n",
       "3  e7a40475-873d-426e-b37d-7f4c0dbd11c3           3    1   1.83   1.83   \n",
       "4  2b89806d-1688-4364-88c6-c1ffc499e63e           4    1   2.50   1.50   \n",
       "\n",
       "   Best of    Comment    Court    Date   L1  ...  one_three_vv  one_five_vv  \\\n",
       "0        3  Completed  Outdoor  1/6/20  3.0  ...           1.0          0.0   \n",
       "1        3  Completed  Outdoor  1/6/20  6.0  ...           1.0          0.0   \n",
       "2        3  Completed  Outdoor  1/6/20  4.0  ...           1.0          0.0   \n",
       "3        3  Completed  Outdoor  1/6/20  6.0  ...           1.0          0.0   \n",
       "4        3  Completed  Outdoor  1/6/20  3.0  ...           1.0          0.0   \n",
       "\n",
       "   pts_diff  player_one_pts  player_two_pts  player_one_total_games  \\\n",
       "0      -192            1111             919                      12   \n",
       "1      -165             803             638                      13   \n",
       "2       158            1025             867                      12   \n",
       "3       224             905             681                      15   \n",
       "4       284             920             636                      15   \n",
       "\n",
       "  player_two_total_games total_sets  player_one_sets player_two_sets  \n",
       "0                      7        2.0              0.0             2.0  \n",
       "1                     10        2.0              0.0             2.0  \n",
       "2                      7        2.0              2.0             0.0  \n",
       "3                     13        3.0              2.0             1.0  \n",
       "4                     12        3.0              2.0             1.0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comment\n",
       "Awarded         0.000035\n",
       "Completed       0.961710\n",
       "Disqualified    0.000035\n",
       "Retired         0.031937\n",
       "Sched           0.000069\n",
       "Walkover        0.006215\n",
       "Name: Comment, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Comment')['Comment'].count()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data.Comment=='Completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'Unnamed: 0', 'ATP', 'B365L', 'B365W', 'Best of', 'Comment',\n",
       "       'Court', 'Date', 'L1', 'L2', 'L3', 'L4', 'L5', 'LPts', 'LRank',\n",
       "       'Location', 'Loser', 'Lsets', 'Round', 'Series', 'Surface',\n",
       "       'Tournament', 'W1', 'W2', 'W3', 'W4', 'W5', 'WPts', 'WRank', 'Winner',\n",
       "       'Wsets', 'rank_diff', 'outcome', 'player_one', 'player_two',\n",
       "       'player_one_rank', 'player_two_rank', 'uuid.1', 'one_name', 'one_date',\n",
       "       'one_cutoff_date', 'one_win_rate_year', 'one_games_played_year',\n",
       "       'one_clay_year', 'one_grass_year', 'one_hard_year', 'one_three_year',\n",
       "       'one_five_year', 'two_name', 'two_date', 'two_cutoff_date',\n",
       "       'two_win_rate_year', 'two_games_played_year', 'two_clay_year',\n",
       "       'two_grass_year', 'two_hard_year', 'two_three_year', 'two_five_year',\n",
       "       'major', 'total_games', 'player_two_name_vv', 'two_cutoff_date_vv',\n",
       "       'two_win_rate_vv', 'two_games_played_vv', 'two_clay_vv', 'two_grass_vv',\n",
       "       'two_hard_vv', 'two_three_vv', 'two_five_vv', 'player_one_name_vv',\n",
       "       'one_cutoff_date_vv', 'one_win_rate_vv', 'one_games_played_vv',\n",
       "       'one_clay_vv', 'one_grass_vv', 'one_hard_vv', 'one_three_vv',\n",
       "       'one_five_vv', 'pts_diff', 'player_one_pts', 'player_two_pts',\n",
       "       'player_one_total_games', 'player_two_total_games', 'total_sets',\n",
       "       'player_one_sets', 'player_two_sets', 'rank_dif', 'pts_dif',\n",
       "       'win_rate_diff', 'hard_diff', 'five_diff', 'three_diff', 'hard_vv',\n",
       "       'vv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of games with only top 100 players:  0.6574639190062469\n"
     ]
    }
   ],
   "source": [
    "print(\"% of games with only top 100 players: \",len(data[(data.WRank<=100) & (data.LRank<=100)])/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome\n",
       "0    0.335715\n",
       "1    0.664285\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of times the higher ranked player one\n",
    "data.groupby('outcome')['outcome'].count()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome\n",
       "0    0.259141\n",
       "1    0.740859\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of times the higher ranked player one in 5set games\n",
    "data[data['Best of']==5].groupby('outcome')['outcome'].count()/len(data[data['Best of']==5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_sets\n",
       "0.0        2\n",
       "1.0        4\n",
       "2.0    14463\n",
       "3.0     8076\n",
       "4.0        1\n",
       "5.0        2\n",
       "Name: total_sets, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of times the higher ranked player one in 5set games\n",
    "data[data['Best of']==3].groupby('total_sets')['total_sets'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ATP</th>\n",
       "      <th>B365L</th>\n",
       "      <th>B365W</th>\n",
       "      <th>Best of</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Court</th>\n",
       "      <th>Date</th>\n",
       "      <th>L1</th>\n",
       "      <th>...</th>\n",
       "      <th>one_three_vv</th>\n",
       "      <th>one_five_vv</th>\n",
       "      <th>pts_diff</th>\n",
       "      <th>player_one_pts</th>\n",
       "      <th>player_two_pts</th>\n",
       "      <th>player_one_total_games</th>\n",
       "      <th>player_two_total_games</th>\n",
       "      <th>total_sets</th>\n",
       "      <th>player_one_sets</th>\n",
       "      <th>player_two_sets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>cb581777-523b-432b-b342-57306d65cbbf</td>\n",
       "      <td>2206</td>\n",
       "      <td>53</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>9/22/19</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>190</td>\n",
       "      <td>955</td>\n",
       "      <td>765</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7533</th>\n",
       "      <td>a14edf1d-42c3-4bc6-92e8-f3b51bc7440d</td>\n",
       "      <td>2229</td>\n",
       "      <td>54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>9/24/17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-541</td>\n",
       "      <td>1085</td>\n",
       "      <td>544</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10460</th>\n",
       "      <td>562bb080-8010-44e0-9fc4-2c63febfb817</td>\n",
       "      <td>2532</td>\n",
       "      <td>63</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>10/30/16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-1960</td>\n",
       "      <td>4650</td>\n",
       "      <td>2690</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13078</th>\n",
       "      <td>61de4009-c143-4df0-b53b-d6065966da91</td>\n",
       "      <td>2532</td>\n",
       "      <td>63</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>10/30/16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-1960</td>\n",
       "      <td>4650</td>\n",
       "      <td>2690</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uuid  Unnamed: 0  ATP  B365L  B365W  \\\n",
       "2269   cb581777-523b-432b-b342-57306d65cbbf        2206   53   3.00    1.4   \n",
       "7533   a14edf1d-42c3-4bc6-92e8-f3b51bc7440d        2229   54   1.53    2.5   \n",
       "10460  562bb080-8010-44e0-9fc4-2c63febfb817        2532   63   1.61    2.3   \n",
       "13078  61de4009-c143-4df0-b53b-d6065966da91        2532   63   1.61    2.3   \n",
       "\n",
       "       Best of    Comment   Court      Date   L1  ...  one_three_vv  \\\n",
       "2269         3  Completed  Indoor   9/22/19  7.0  ...           1.0   \n",
       "7533         3  Completed  Indoor   9/24/17  5.0  ...           0.5   \n",
       "10460        3  Completed  Indoor  10/30/16  1.0  ...           0.8   \n",
       "13078        3  Completed  Indoor  10/30/16  1.0  ...           0.8   \n",
       "\n",
       "       one_five_vv  pts_diff  player_one_pts  player_two_pts  \\\n",
       "2269      0.000000       190             955             765   \n",
       "7533      0.000000      -541            1085             544   \n",
       "10460     0.666667     -1960            4650            2690   \n",
       "13078     0.666667     -1960            4650            2690   \n",
       "\n",
       "       player_one_total_games player_two_total_games total_sets  \\\n",
       "2269                        9                     11        1.0   \n",
       "7533                       13                      7        1.0   \n",
       "10460                      13                      7        1.0   \n",
       "13078                      13                      7        1.0   \n",
       "\n",
       "       player_one_sets player_two_sets  \n",
       "2269               0.0             1.0  \n",
       "7533               0.0             1.0  \n",
       "10460              0.0             1.0  \n",
       "13078              0.0             1.0  \n",
       "\n",
       "[4 rows x 87 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data['Best of']==3) & (data.total_sets==1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "data['rank_dif']=data['player_two_rank']-data['player_one_rank']\n",
    "data['pts_dif']=data['player_two_pts']-data['player_one_pts']\n",
    "data['win_rate_diff']=data['two_win_rate_year']-data['one_win_rate_year']\n",
    "data['hard_diff']=data['two_hard_year']-data['one_hard_year']\n",
    "data['five_diff']=data['two_five_year']-data['one_five_year']\n",
    "data['three_diff']=data['two_three_year']-data['one_three_year']\n",
    "\n",
    "data['hard_vv']=data['two_hard_vv']-data['one_hard_vv']\n",
    "data['vv']=data['two_win_rate_vv']-data['one_win_rate_vv']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model to predict match <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ATP</th>\n",
       "      <th>B365L</th>\n",
       "      <th>B365W</th>\n",
       "      <th>Best of</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Court</th>\n",
       "      <th>Date</th>\n",
       "      <th>L1</th>\n",
       "      <th>...</th>\n",
       "      <th>player_one_sets</th>\n",
       "      <th>player_two_sets</th>\n",
       "      <th>rank_dif</th>\n",
       "      <th>pts_dif</th>\n",
       "      <th>win_rate_diff</th>\n",
       "      <th>hard_diff</th>\n",
       "      <th>five_diff</th>\n",
       "      <th>three_diff</th>\n",
       "      <th>hard_vv</th>\n",
       "      <th>vv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73f14b1f-a9af-4ef9-88a0-df3b4ff82e12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12</td>\n",
       "      <td>-192</td>\n",
       "      <td>0.124772</td>\n",
       "      <td>0.102871</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77cb58d6-44ed-4875-a622-4c6839b08072</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>-165</td>\n",
       "      <td>-0.033871</td>\n",
       "      <td>-0.021645</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.097403</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9bee2c88-a8e5-4169-b0d6-5ad4a030d689</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>-158</td>\n",
       "      <td>0.032741</td>\n",
       "      <td>-0.115468</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>0.044974</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7a40475-873d-426e-b37d-7f4c0dbd11c3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.83</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>-224</td>\n",
       "      <td>0.110256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2b89806d-1688-4364-88c6-c1ffc499e63e</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>1/6/20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "      <td>-284</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>-0.214286</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  Unnamed: 0  ATP  B365L  B365W  \\\n",
       "0  73f14b1f-a9af-4ef9-88a0-df3b4ff82e12           0    1   1.72   2.00   \n",
       "1  77cb58d6-44ed-4875-a622-4c6839b08072           1    1   2.25   1.57   \n",
       "2  9bee2c88-a8e5-4169-b0d6-5ad4a030d689           2    1   3.75   1.25   \n",
       "3  e7a40475-873d-426e-b37d-7f4c0dbd11c3           3    1   1.83   1.83   \n",
       "4  2b89806d-1688-4364-88c6-c1ffc499e63e           4    1   2.50   1.50   \n",
       "\n",
       "   Best of    Comment    Court    Date   L1  ...  player_one_sets  \\\n",
       "0        3  Completed  Outdoor  1/6/20  3.0  ...              0.0   \n",
       "1        3  Completed  Outdoor  1/6/20  6.0  ...              0.0   \n",
       "2        3  Completed  Outdoor  1/6/20  4.0  ...              2.0   \n",
       "3        3  Completed  Outdoor  1/6/20  6.0  ...              2.0   \n",
       "4        3  Completed  Outdoor  1/6/20  3.0  ...              2.0   \n",
       "\n",
       "   player_two_sets  rank_dif  pts_dif  win_rate_diff  hard_diff five_diff  \\\n",
       "0              2.0        12     -192       0.124772   0.102871  0.033333   \n",
       "1              2.0        13     -165      -0.033871  -0.021645  0.111111   \n",
       "2              0.0        15     -158       0.032741  -0.115468 -0.012987   \n",
       "3              1.0        18     -224       0.110256   0.000000  0.166667   \n",
       "4              1.0        29     -284       0.072727   0.003030 -0.214286   \n",
       "\n",
       "  three_diff  hard_vv   vv  \n",
       "0   0.117500     -1.0 -1.0  \n",
       "1  -0.097403     -1.0 -1.0  \n",
       "2   0.044974     -1.0 -1.0  \n",
       "3   0.079545     -1.0 -1.0  \n",
       "4   0.166667     -1.0 -1.0  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25290, 42)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_columns=['player_one_rank','player_two_rank','Surface', 'Best of','one_win_rate_year',\n",
    "       'one_games_played_year', 'one_clay_year', 'one_grass_year',\n",
    "       'one_hard_year', 'one_three_year', 'one_five_year', 'two_win_rate_year',\n",
    "       'two_games_played_year', 'two_clay_year', 'two_grass_year',\n",
    "       'two_hard_year', 'two_three_year', 'two_five_year', 'major','one_win_rate_vv', 'one_games_played_vv', 'one_clay_vv', 'one_grass_vv',\n",
    "       'one_hard_vv', 'one_three_vv', 'one_five_vv','two_win_rate_vv',\n",
    "       'two_games_played_vv', 'two_clay_vv', 'two_grass_vv', 'two_hard_vv',\n",
    "       'two_three_vv', 'two_five_vv', 'rank_dif', 'win_rate_diff', 'hard_diff',\n",
    "       'five_diff', 'three_diff', 'hard_vv', 'vv' ,'pts_diff', 'outcome']\n",
    "data=data[feature_columns].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='outcome'\n",
    "y=data[target]\n",
    "X=data.drop('outcome',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    (['player_one_rank'], [StandardScaler(),SimpleImputer()]),\n",
    "    (['player_two_rank'], [StandardScaler(),SimpleImputer()]),\n",
    "    (['Surface'], [SimpleImputer(strategy='constant', fill_value='most_frequent'),LabelBinarizer()]),\n",
    "    ('Best of', LabelEncoder()),\n",
    "    (['one_win_rate_year'],StandardScaler()),\n",
    "    (['one_games_played_year'], StandardScaler()),\n",
    "    (['one_clay_year'], StandardScaler()),\n",
    "    (['one_grass_year'],StandardScaler()),\n",
    "    (['one_hard_year'], StandardScaler()),\n",
    "    (['one_three_year'], StandardScaler()),\n",
    "    (['one_five_year'], StandardScaler()),\n",
    "    (['two_win_rate_year'],StandardScaler()),\n",
    "    (['two_games_played_year'], StandardScaler()),\n",
    "    (['two_clay_year'], StandardScaler()),\n",
    "    (['two_grass_year'],StandardScaler()),\n",
    "    (['two_hard_year'], StandardScaler()),\n",
    "    (['two_three_year'], StandardScaler()),\n",
    "    (['two_five_year'], StandardScaler()),\n",
    "    ('major', LabelEncoder()),\n",
    "    (['one_win_rate_vv'],StandardScaler()), \n",
    "    (['one_games_played_vv'],StandardScaler()),  \n",
    "    (['one_clay_vv'],StandardScaler()),  \n",
    "    (['one_grass_vv'],StandardScaler()), \n",
    "    (['one_hard_vv'],StandardScaler()),  \n",
    "    (['one_three_vv'],StandardScaler()),  \n",
    "    (['one_five_vv'],StandardScaler()),\n",
    "    (['two_win_rate_vv'],StandardScaler()), \n",
    "    (['two_games_played_vv'],StandardScaler()),  \n",
    "    (['two_clay_vv'],StandardScaler()),  \n",
    "    (['two_grass_vv'],StandardScaler()), \n",
    "    (['two_hard_vv'],StandardScaler()),  \n",
    "    (['two_three_vv'],StandardScaler()),  \n",
    "    (['two_five_vv'],StandardScaler()),    \n",
    "    (['rank_dif'],StandardScaler()),  \n",
    "    #(['pts_diff'],[StandardScaler(),SimpleImputer()]),  \n",
    "    (['win_rate_diff'],StandardScaler()),  \n",
    "    (['hard_diff'],StandardScaler()),\n",
    "    (['five_diff'],StandardScaler()), \n",
    "    (['three_diff'],StandardScaler()),\n",
    "    (['hard_vv'],StandardScaler()),\n",
    "    (['vv'],StandardScaler())\n",
    "    \n",
    "], df_out=\n",
    "    True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 256 ms, sys: 32.7 ms, total: 288 ms\n",
      "Wall time: 305 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Z_train=mapper.fit_transform(X_train)\n",
    "Z_test=mapper.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(C=20, max_iter=1000, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=20, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Z_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6916271253459866"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(Z_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6795175958877027, 0.5935484408246203)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred),roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([['two_three_year', 'two_games_played_year', 'three_diff', 'major', 'one_three_year', 'Surface_Clay', 'player_one_rank', 'one_games_played_vv', 'two_games_played_vv', 'hard_vv', 'Surface_Hard', 'two_grass_vv', 'two_grass_year', 'one_grass_vv', 'two_clay_year', 'two_hard_year', 'hard_diff', 'one_win_rate_vv', 'five_diff', 'two_five_year', 'Best of', 'one_clay_year', 'one_hard_year', 'two_five_vv', 'one_five_year', 'one_three_vv', 'two_clay_vv', 'vv', 'two_win_rate_vv', 'two_three_vv', 'one_grass_year', 'two_hard_vv', 'one_five_vv', 'player_two_rank', 'rank_dif', 'one_hard_vv', 'Surface_Grass', 'one_clay_vv', 'win_rate_diff', 'one_games_played_year', 'one_win_rate_year', 'two_win_rate_year', 'pts_diff']], dtype='object')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_train.columns[model.coef_.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(units=64, activation='relu', input_shape=(Z_train.shape[1],)))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Dense(units=32, activation='relu'))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Dense(units=16, activation='relu'))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Dense(units=5, activation='relu'))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "mc=ModelCheckpoint('data/best_model.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20232 samples, validate on 5058 samples\n",
      "Epoch 1/175\n",
      "20096/20232 [============================>.] - ETA: 0s - loss: 0.6127 - accuracy: 0.6554\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.66074, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 1s 52us/sample - loss: 0.6127 - accuracy: 0.6551 - val_loss: 0.5618 - val_accuracy: 0.6607\n",
      "Epoch 2/175\n",
      "17920/20232 [=========================>....] - ETA: 0s - loss: 0.5763 - accuracy: 0.6742\n",
      "Epoch 00002: val_accuracy improved from 0.66074 to 0.68031, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.5768 - accuracy: 0.6734 - val_loss: 0.5528 - val_accuracy: 0.6803\n",
      "Epoch 3/175\n",
      "19712/20232 [============================>.] - ETA: 0s - loss: 0.5661 - accuracy: 0.6841\n",
      "Epoch 00003: val_accuracy improved from 0.68031 to 0.68901, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.5658 - accuracy: 0.6844 - val_loss: 0.5459 - val_accuracy: 0.6890\n",
      "Epoch 4/175\n",
      "17408/20232 [========================>.....] - ETA: 0s - loss: 0.5586 - accuracy: 0.6893\n",
      "Epoch 00004: val_accuracy improved from 0.68901 to 0.69316, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.5578 - accuracy: 0.6914 - val_loss: 0.5410 - val_accuracy: 0.6932\n",
      "Epoch 5/175\n",
      "17792/20232 [=========================>....] - ETA: 0s - loss: 0.5530 - accuracy: 0.6891\n",
      "Epoch 00005: val_accuracy improved from 0.69316 to 0.70324, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 1s 25us/sample - loss: 0.5524 - accuracy: 0.6898 - val_loss: 0.5367 - val_accuracy: 0.7032\n",
      "Epoch 6/175\n",
      "16640/20232 [=======================>......] - ETA: 0s - loss: 0.5490 - accuracy: 0.6941\n",
      "Epoch 00006: val_accuracy did not improve from 0.70324\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.5515 - accuracy: 0.6914 - val_loss: 0.5364 - val_accuracy: 0.6991\n",
      "Epoch 7/175\n",
      "16896/20232 [========================>.....] - ETA: 0s - loss: 0.5426 - accuracy: 0.6953\n",
      "Epoch 00007: val_accuracy improved from 0.70324 to 0.70403, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.5461 - accuracy: 0.6927 - val_loss: 0.5337 - val_accuracy: 0.7040\n",
      "Epoch 8/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.5437 - accuracy: 0.6945 ETA: 0s - loss: 0.5378 - accuracy: \n",
      "Epoch 00008: val_accuracy improved from 0.70403 to 0.70482, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.5433 - accuracy: 0.6950 - val_loss: 0.5309 - val_accuracy: 0.7048\n",
      "Epoch 9/175\n",
      "17664/20232 [=========================>....] - ETA: 0s - loss: 0.5409 - accuracy: 0.6992\n",
      "Epoch 00009: val_accuracy improved from 0.70482 to 0.70838, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 24us/sample - loss: 0.5413 - accuracy: 0.6983 - val_loss: 0.5297 - val_accuracy: 0.7084\n",
      "Epoch 10/175\n",
      "18560/20232 [==========================>...] - ETA: 0s - loss: 0.5390 - accuracy: 0.7001\n",
      "Epoch 00010: val_accuracy improved from 0.70838 to 0.71135, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 1s 30us/sample - loss: 0.5391 - accuracy: 0.6999 - val_loss: 0.5274 - val_accuracy: 0.7113\n",
      "Epoch 11/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.5341 - accuracy: 0.7054\n",
      "Epoch 00011: val_accuracy improved from 0.71135 to 0.71234, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.5344 - accuracy: 0.7047 - val_loss: 0.5253 - val_accuracy: 0.7123\n",
      "Epoch 12/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.5296 - accuracy: 0.7065 ETA: 0s - loss: 0.5288 - accuracy: 0.70\n",
      "Epoch 00012: val_accuracy did not improve from 0.71234\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.5303 - accuracy: 0.7054 - val_loss: 0.5278 - val_accuracy: 0.7098\n",
      "Epoch 13/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.5322 - accuracy: 0.7058\n",
      "Epoch 00013: val_accuracy did not improve from 0.71234\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.5306 - accuracy: 0.7060 - val_loss: 0.5271 - val_accuracy: 0.7094\n",
      "Epoch 14/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.5294 - accuracy: 0.7068\n",
      "Epoch 00014: val_accuracy improved from 0.71234 to 0.71313, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.5285 - accuracy: 0.7076 - val_loss: 0.5222 - val_accuracy: 0.7131\n",
      "Epoch 15/175\n",
      "18560/20232 [==========================>...] - ETA: 0s - loss: 0.5313 - accuracy: 0.7059\n",
      "Epoch 00015: val_accuracy did not improve from 0.71313\n",
      "20232/20232 [==============================] - 1s 29us/sample - loss: 0.5284 - accuracy: 0.7070 - val_loss: 0.5217 - val_accuracy: 0.7123\n",
      "Epoch 16/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.5283 - accuracy: 0.7062\n",
      "Epoch 00016: val_accuracy improved from 0.71313 to 0.71530, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 24us/sample - loss: 0.5275 - accuracy: 0.7070 - val_loss: 0.5200 - val_accuracy: 0.7153\n",
      "Epoch 17/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.5264 - accuracy: 0.7083\n",
      "Epoch 00017: val_accuracy did not improve from 0.71530\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.5263 - accuracy: 0.7081 - val_loss: 0.5204 - val_accuracy: 0.7149\n",
      "Epoch 18/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.5236 - accuracy: 0.7112\n",
      "Epoch 00018: val_accuracy did not improve from 0.71530\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.5240 - accuracy: 0.7109 - val_loss: 0.5191 - val_accuracy: 0.7143\n",
      "Epoch 19/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.5198 - accuracy: 0.7112\n",
      "Epoch 00019: val_accuracy improved from 0.71530 to 0.71570, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.5202 - accuracy: 0.7103 - val_loss: 0.5166 - val_accuracy: 0.7157\n",
      "Epoch 20/175\n",
      "19584/20232 [============================>.] - ETA: 0s - loss: 0.5200 - accuracy: 0.7172\n",
      "Epoch 00020: val_accuracy did not improve from 0.71570\n",
      "20232/20232 [==============================] - 0s 23us/sample - loss: 0.5194 - accuracy: 0.7178 - val_loss: 0.5172 - val_accuracy: 0.7131\n",
      "Epoch 21/175\n",
      "19200/20232 [===========================>..] - ETA: 0s - loss: 0.5197 - accuracy: 0.7131\n",
      "Epoch 00021: val_accuracy improved from 0.71570 to 0.71708, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 1s 26us/sample - loss: 0.5199 - accuracy: 0.7122 - val_loss: 0.5170 - val_accuracy: 0.7171\n",
      "Epoch 22/175\n",
      "19584/20232 [============================>.] - ETA: 0s - loss: 0.5171 - accuracy: 0.7160\n",
      "Epoch 00022: val_accuracy improved from 0.71708 to 0.72143, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.5165 - accuracy: 0.7163 - val_loss: 0.5159 - val_accuracy: 0.7214\n",
      "Epoch 23/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.5190 - accuracy: 0.7112\n",
      "Epoch 00023: val_accuracy did not improve from 0.72143\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.5189 - accuracy: 0.7114 - val_loss: 0.5174 - val_accuracy: 0.7171\n",
      "Epoch 24/175\n",
      "20224/20232 [============================>.] - ETA: 0s - loss: 0.5130 - accuracy: 0.7172\n",
      "Epoch 00024: val_accuracy did not improve from 0.72143\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.5129 - accuracy: 0.7172 - val_loss: 0.5153 - val_accuracy: 0.7159\n",
      "Epoch 25/175\n",
      "17664/20232 [=========================>....] - ETA: 0s - loss: 0.5123 - accuracy: 0.7152\n",
      "Epoch 00025: val_accuracy did not improve from 0.72143\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.5140 - accuracy: 0.7143 - val_loss: 0.5146 - val_accuracy: 0.7133\n",
      "Epoch 26/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.5150 - accuracy: 0.7143\n",
      "Epoch 00026: val_accuracy did not improve from 0.72143\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.5152 - accuracy: 0.7138 - val_loss: 0.5135 - val_accuracy: 0.7157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/175\n",
      "19840/20232 [============================>.] - ETA: 0s - loss: 0.5109 - accuracy: 0.7189\n",
      "Epoch 00027: val_accuracy did not improve from 0.72143\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.5115 - accuracy: 0.7182 - val_loss: 0.5136 - val_accuracy: 0.7167\n",
      "Epoch 28/175\n",
      "17408/20232 [========================>.....] - ETA: 0s - loss: 0.5114 - accuracy: 0.7199\n",
      "Epoch 00028: val_accuracy did not improve from 0.72143\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.5114 - accuracy: 0.7189 - val_loss: 0.5141 - val_accuracy: 0.7175\n",
      "Epoch 29/175\n",
      "19328/20232 [===========================>..] - ETA: 0s - loss: 0.5109 - accuracy: 0.7167\n",
      "Epoch 00029: val_accuracy did not improve from 0.72143\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.5109 - accuracy: 0.7162 - val_loss: 0.5137 - val_accuracy: 0.7165\n",
      "Epoch 30/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.5096 - accuracy: 0.7180\n",
      "Epoch 00030: val_accuracy did not improve from 0.72143\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.5101 - accuracy: 0.7177 - val_loss: 0.5139 - val_accuracy: 0.7171\n",
      "Epoch 31/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.5096 - accuracy: 0.7188\n",
      "Epoch 00031: val_accuracy improved from 0.72143 to 0.72222, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.5087 - accuracy: 0.7198 - val_loss: 0.5117 - val_accuracy: 0.7222\n",
      "Epoch 32/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.5098 - accuracy: 0.7148\n",
      "Epoch 00032: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.5096 - accuracy: 0.7155 - val_loss: 0.5134 - val_accuracy: 0.7197\n",
      "Epoch 33/175\n",
      "19840/20232 [============================>.] - ETA: 0s - loss: 0.5074 - accuracy: 0.7210\n",
      "Epoch 00033: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.5074 - accuracy: 0.7209 - val_loss: 0.5114 - val_accuracy: 0.7195\n",
      "Epoch 34/175\n",
      "19328/20232 [===========================>..] - ETA: 0s - loss: 0.5056 - accuracy: 0.7178\n",
      "Epoch 00034: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.5047 - accuracy: 0.7196 - val_loss: 0.5130 - val_accuracy: 0.7195\n",
      "Epoch 35/175\n",
      "17536/20232 [=========================>....] - ETA: 0s - loss: 0.5063 - accuracy: 0.7186\n",
      "Epoch 00035: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.5062 - accuracy: 0.7188 - val_loss: 0.5116 - val_accuracy: 0.7189\n",
      "Epoch 36/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.5059 - accuracy: 0.7189\n",
      "Epoch 00036: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.5043 - accuracy: 0.7204 - val_loss: 0.5149 - val_accuracy: 0.7149\n",
      "Epoch 37/175\n",
      "17792/20232 [=========================>....] - ETA: 0s - loss: 0.5063 - accuracy: 0.7198\n",
      "Epoch 00037: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.5051 - accuracy: 0.7201 - val_loss: 0.5133 - val_accuracy: 0.7191\n",
      "Epoch 38/175\n",
      "18048/20232 [=========================>....] - ETA: 0s - loss: 0.5010 - accuracy: 0.7232\n",
      "Epoch 00038: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.5045 - accuracy: 0.7196 - val_loss: 0.5112 - val_accuracy: 0.7161\n",
      "Epoch 39/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.5011 - accuracy: 0.7218\n",
      "Epoch 00039: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.5026 - accuracy: 0.7214 - val_loss: 0.5116 - val_accuracy: 0.7218\n",
      "Epoch 40/175\n",
      "17920/20232 [=========================>....] - ETA: 0s - loss: 0.5037 - accuracy: 0.7209\n",
      "Epoch 00040: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.5039 - accuracy: 0.7202 - val_loss: 0.5108 - val_accuracy: 0.7220\n",
      "Epoch 41/175\n",
      "19712/20232 [============================>.] - ETA: 0s - loss: 0.5032 - accuracy: 0.7221\n",
      "Epoch 00041: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.5029 - accuracy: 0.7223 - val_loss: 0.5102 - val_accuracy: 0.7171\n",
      "Epoch 42/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.5041 - accuracy: 0.7238\n",
      "Epoch 00042: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.5032 - accuracy: 0.7242 - val_loss: 0.5107 - val_accuracy: 0.7220\n",
      "Epoch 43/175\n",
      "19840/20232 [============================>.] - ETA: 0s - loss: 0.5026 - accuracy: 0.7235\n",
      "Epoch 00043: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.5021 - accuracy: 0.7235 - val_loss: 0.5093 - val_accuracy: 0.7222\n",
      "Epoch 44/175\n",
      "17408/20232 [========================>.....] - ETA: 0s - loss: 0.4968 - accuracy: 0.7268\n",
      "Epoch 00044: val_accuracy did not improve from 0.72222\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4978 - accuracy: 0.7273 - val_loss: 0.5093 - val_accuracy: 0.7198\n",
      "Epoch 45/175\n",
      "19712/20232 [============================>.] - ETA: 0s - loss: 0.4973 - accuracy: 0.7267\n",
      "Epoch 00045: val_accuracy improved from 0.72222 to 0.72361, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 1s 30us/sample - loss: 0.4979 - accuracy: 0.7264 - val_loss: 0.5098 - val_accuracy: 0.7236\n",
      "Epoch 46/175\n",
      "19840/20232 [============================>.] - ETA: 0s - loss: 0.4997 - accuracy: 0.7254\n",
      "Epoch 00046: val_accuracy improved from 0.72361 to 0.72380, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 1s 38us/sample - loss: 0.5000 - accuracy: 0.7255 - val_loss: 0.5088 - val_accuracy: 0.7238\n",
      "Epoch 47/175\n",
      "16512/20232 [=======================>......] - ETA: 0s - loss: 0.4980 - accuracy: 0.7227\n",
      "Epoch 00047: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4998 - accuracy: 0.7231 - val_loss: 0.5097 - val_accuracy: 0.7228\n",
      "Epoch 48/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.5007 - accuracy: 0.7272\n",
      "Epoch 00048: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 23us/sample - loss: 0.5000 - accuracy: 0.7271 - val_loss: 0.5102 - val_accuracy: 0.7236\n",
      "Epoch 49/175\n",
      "17792/20232 [=========================>....] - ETA: 0s - loss: 0.4939 - accuracy: 0.7248\n",
      "Epoch 00049: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 24us/sample - loss: 0.4948 - accuracy: 0.7256 - val_loss: 0.5105 - val_accuracy: 0.7214\n",
      "Epoch 50/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4977 - accuracy: 0.7237\n",
      "Epoch 00050: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4978 - accuracy: 0.7238 - val_loss: 0.5106 - val_accuracy: 0.7218\n",
      "Epoch 51/175\n",
      "19584/20232 [============================>.] - ETA: 0s - loss: 0.4950 - accuracy: 0.7279\n",
      "Epoch 00051: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4947 - accuracy: 0.7282 - val_loss: 0.5092 - val_accuracy: 0.7228\n",
      "Epoch 52/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.4955 - accuracy: 0.7280\n",
      "Epoch 00052: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4969 - accuracy: 0.7271 - val_loss: 0.5108 - val_accuracy: 0.7226\n",
      "Epoch 53/175\n",
      "18048/20232 [=========================>....] - ETA: 0s - loss: 0.4971 - accuracy: 0.7231\n",
      "Epoch 00053: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4971 - accuracy: 0.7236 - val_loss: 0.5103 - val_accuracy: 0.7208\n",
      "Epoch 54/175\n",
      "17152/20232 [========================>.....] - ETA: 0s - loss: 0.4938 - accuracy: 0.7304\n",
      "Epoch 00054: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4942 - accuracy: 0.7297 - val_loss: 0.5122 - val_accuracy: 0.7206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4939 - accuracy: 0.7385\n",
      "Epoch 00055: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4940 - accuracy: 0.7386 - val_loss: 0.5110 - val_accuracy: 0.7191\n",
      "Epoch 56/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.4920 - accuracy: 0.7443\n",
      "Epoch 00056: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4913 - accuracy: 0.7440 - val_loss: 0.5112 - val_accuracy: 0.7185\n",
      "Epoch 57/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4938 - accuracy: 0.7425\n",
      "Epoch 00057: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4938 - accuracy: 0.7432 - val_loss: 0.5107 - val_accuracy: 0.7216\n",
      "Epoch 58/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4918 - accuracy: 0.7419\n",
      "Epoch 00058: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4919 - accuracy: 0.7419 - val_loss: 0.5103 - val_accuracy: 0.7212\n",
      "Epoch 59/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4897 - accuracy: 0.7441\n",
      "Epoch 00059: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4910 - accuracy: 0.7434 - val_loss: 0.5095 - val_accuracy: 0.7228\n",
      "Epoch 60/175\n",
      "18432/20232 [==========================>...] - ETA: 0s - loss: 0.4889 - accuracy: 0.7450\n",
      "Epoch 00060: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4897 - accuracy: 0.7447 - val_loss: 0.5099 - val_accuracy: 0.7195\n",
      "Epoch 61/175\n",
      "17152/20232 [========================>.....] - ETA: 0s - loss: 0.4863 - accuracy: 0.7461\n",
      "Epoch 00061: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4889 - accuracy: 0.7425 - val_loss: 0.5105 - val_accuracy: 0.7175\n",
      "Epoch 62/175\n",
      "18048/20232 [=========================>....] - ETA: 0s - loss: 0.4920 - accuracy: 0.7430\n",
      "Epoch 00062: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4910 - accuracy: 0.7432 - val_loss: 0.5101 - val_accuracy: 0.7165\n",
      "Epoch 63/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4916 - accuracy: 0.7474\n",
      "Epoch 00063: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4892 - accuracy: 0.7484 - val_loss: 0.5098 - val_accuracy: 0.7224\n",
      "Epoch 64/175\n",
      "19200/20232 [===========================>..] - ETA: 0s - loss: 0.4900 - accuracy: 0.7452\n",
      "Epoch 00064: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4907 - accuracy: 0.7456 - val_loss: 0.5088 - val_accuracy: 0.7169\n",
      "Epoch 65/175\n",
      "19584/20232 [============================>.] - ETA: 0s - loss: 0.4913 - accuracy: 0.7467\n",
      "Epoch 00065: val_accuracy did not improve from 0.72380\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4903 - accuracy: 0.7477 - val_loss: 0.5095 - val_accuracy: 0.7230\n",
      "Epoch 66/175\n",
      "19328/20232 [===========================>..] - ETA: 0s - loss: 0.4851 - accuracy: 0.7502\n",
      "Epoch 00066: val_accuracy improved from 0.72380 to 0.72420, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4859 - accuracy: 0.7494 - val_loss: 0.5075 - val_accuracy: 0.7242\n",
      "Epoch 67/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.4867 - accuracy: 0.7483\n",
      "Epoch 00067: val_accuracy improved from 0.72420 to 0.72855, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4867 - accuracy: 0.7484 - val_loss: 0.5065 - val_accuracy: 0.7285\n",
      "Epoch 68/175\n",
      "17536/20232 [=========================>....] - ETA: 0s - loss: 0.4862 - accuracy: 0.7482\n",
      "Epoch 00068: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4878 - accuracy: 0.7473 - val_loss: 0.5067 - val_accuracy: 0.7256\n",
      "Epoch 69/175\n",
      "17024/20232 [========================>.....] - ETA: 0s - loss: 0.4864 - accuracy: 0.7475\n",
      "Epoch 00069: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4877 - accuracy: 0.7468 - val_loss: 0.5077 - val_accuracy: 0.7230\n",
      "Epoch 70/175\n",
      "16896/20232 [========================>.....] - ETA: 0s - loss: 0.4859 - accuracy: 0.7495\n",
      "Epoch 00070: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4875 - accuracy: 0.7478 - val_loss: 0.5078 - val_accuracy: 0.7200\n",
      "Epoch 71/175\n",
      "17408/20232 [========================>.....] - ETA: 0s - loss: 0.4859 - accuracy: 0.7510\n",
      "Epoch 00071: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4859 - accuracy: 0.7497 - val_loss: 0.5081 - val_accuracy: 0.7242\n",
      "Epoch 72/175\n",
      "18688/20232 [==========================>...] - ETA: 0s - loss: 0.4882 - accuracy: 0.7504\n",
      "Epoch 00072: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4875 - accuracy: 0.7512 - val_loss: 0.5053 - val_accuracy: 0.7238\n",
      "Epoch 73/175\n",
      "18560/20232 [==========================>...] - ETA: 0s - loss: 0.4857 - accuracy: 0.7487\n",
      "Epoch 00073: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4870 - accuracy: 0.7468 - val_loss: 0.5066 - val_accuracy: 0.7258\n",
      "Epoch 74/175\n",
      "17536/20232 [=========================>....] - ETA: 0s - loss: 0.4864 - accuracy: 0.7509\n",
      "Epoch 00074: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4863 - accuracy: 0.7514 - val_loss: 0.5083 - val_accuracy: 0.7230\n",
      "Epoch 75/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.4880 - accuracy: 0.7520\n",
      "Epoch 00075: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4866 - accuracy: 0.7527 - val_loss: 0.5091 - val_accuracy: 0.7193\n",
      "Epoch 76/175\n",
      "19200/20232 [===========================>..] - ETA: 0s - loss: 0.4849 - accuracy: 0.7531\n",
      "Epoch 00076: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4847 - accuracy: 0.7526 - val_loss: 0.5073 - val_accuracy: 0.7230\n",
      "Epoch 77/175\n",
      "17408/20232 [========================>.....] - ETA: 0s - loss: 0.4846 - accuracy: 0.7541\n",
      "Epoch 00077: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4856 - accuracy: 0.7523 - val_loss: 0.5078 - val_accuracy: 0.7248\n",
      "Epoch 78/175\n",
      "17792/20232 [=========================>....] - ETA: 0s - loss: 0.4853 - accuracy: 0.7513\n",
      "Epoch 00078: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4847 - accuracy: 0.7533 - val_loss: 0.5076 - val_accuracy: 0.7185\n",
      "Epoch 79/175\n",
      "17024/20232 [========================>.....] - ETA: 0s - loss: 0.4839 - accuracy: 0.7484\n",
      "Epoch 00079: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4853 - accuracy: 0.7501 - val_loss: 0.5092 - val_accuracy: 0.7226\n",
      "Epoch 80/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.4818 - accuracy: 0.7561\n",
      "Epoch 00080: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4826 - accuracy: 0.7552 - val_loss: 0.5067 - val_accuracy: 0.7224\n",
      "Epoch 81/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.4831 - accuracy: 0.7512\n",
      "Epoch 00081: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4831 - accuracy: 0.7511 - val_loss: 0.5090 - val_accuracy: 0.7197\n",
      "Epoch 82/175\n",
      "18688/20232 [==========================>...] - ETA: 0s - loss: 0.4805 - accuracy: 0.7553\n",
      "Epoch 00082: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4816 - accuracy: 0.7550 - val_loss: 0.5093 - val_accuracy: 0.7198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.4849 - accuracy: 0.7519\n",
      "Epoch 00083: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4855 - accuracy: 0.7514 - val_loss: 0.5056 - val_accuracy: 0.7250\n",
      "Epoch 84/175\n",
      "20224/20232 [============================>.] - ETA: 0s - loss: 0.4779 - accuracy: 0.7528\n",
      "Epoch 00084: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4779 - accuracy: 0.7528 - val_loss: 0.5104 - val_accuracy: 0.7165\n",
      "Epoch 85/175\n",
      "19584/20232 [============================>.] - ETA: 0s - loss: 0.4874 - accuracy: 0.7507\n",
      "Epoch 00085: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4860 - accuracy: 0.7522 - val_loss: 0.5056 - val_accuracy: 0.7177\n",
      "Epoch 86/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4786 - accuracy: 0.7566\n",
      "Epoch 00086: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4827 - accuracy: 0.7560 - val_loss: 0.5063 - val_accuracy: 0.7236\n",
      "Epoch 87/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4820 - accuracy: 0.7499\n",
      "Epoch 00087: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4828 - accuracy: 0.7491 - val_loss: 0.5067 - val_accuracy: 0.7224\n",
      "Epoch 88/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4787 - accuracy: 0.7533\n",
      "Epoch 00088: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4795 - accuracy: 0.7534 - val_loss: 0.5053 - val_accuracy: 0.7252\n",
      "Epoch 89/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.4783 - accuracy: 0.7534\n",
      "Epoch 00089: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4796 - accuracy: 0.7528 - val_loss: 0.5084 - val_accuracy: 0.7202\n",
      "Epoch 90/175\n",
      "18688/20232 [==========================>...] - ETA: 0s - loss: 0.4800 - accuracy: 0.7566\n",
      "Epoch 00090: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4797 - accuracy: 0.7557 - val_loss: 0.5076 - val_accuracy: 0.7195\n",
      "Epoch 91/175\n",
      "18688/20232 [==========================>...] - ETA: 0s - loss: 0.4819 - accuracy: 0.7563\n",
      "Epoch 00091: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4804 - accuracy: 0.7571 - val_loss: 0.5064 - val_accuracy: 0.7181\n",
      "Epoch 92/175\n",
      "19712/20232 [============================>.] - ETA: 0s - loss: 0.4816 - accuracy: 0.7543\n",
      "Epoch 00092: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4818 - accuracy: 0.7538 - val_loss: 0.5069 - val_accuracy: 0.7200\n",
      "Epoch 93/175\n",
      "18048/20232 [=========================>....] - ETA: 0s - loss: 0.4794 - accuracy: 0.7502\n",
      "Epoch 00093: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.4795 - accuracy: 0.7509 - val_loss: 0.5037 - val_accuracy: 0.7224\n",
      "Epoch 94/175\n",
      "18688/20232 [==========================>...] - ETA: 0s - loss: 0.4805 - accuracy: 0.75 - ETA: 0s - loss: 0.4795 - accuracy: 0.7536\n",
      "Epoch 00094: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.4800 - accuracy: 0.7542 - val_loss: 0.5065 - val_accuracy: 0.7236\n",
      "Epoch 95/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4794 - accuracy: 0.7545\n",
      "Epoch 00095: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4807 - accuracy: 0.7536 - val_loss: 0.5039 - val_accuracy: 0.7202\n",
      "Epoch 96/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.4779 - accuracy: 0.7520\n",
      "Epoch 00096: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 38us/sample - loss: 0.4777 - accuracy: 0.7517 - val_loss: 0.5039 - val_accuracy: 0.7224\n",
      "Epoch 97/175\n",
      "19840/20232 [============================>.] - ETA: 0s - loss: 0.4813 - accuracy: 0.7536\n",
      "Epoch 00097: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 33us/sample - loss: 0.4813 - accuracy: 0.7532 - val_loss: 0.5021 - val_accuracy: 0.7260\n",
      "Epoch 98/175\n",
      "17792/20232 [=========================>....] - ETA: 0s - loss: 0.4808 - accuracy: 0.7530\n",
      "Epoch 00098: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 23us/sample - loss: 0.4821 - accuracy: 0.7549 - val_loss: 0.5036 - val_accuracy: 0.7224\n",
      "Epoch 99/175\n",
      "19584/20232 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.7563\n",
      "Epoch 00099: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 30us/sample - loss: 0.4775 - accuracy: 0.7559 - val_loss: 0.5035 - val_accuracy: 0.7254\n",
      "Epoch 100/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4790 - accuracy: 0.7557\n",
      "Epoch 00100: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 23us/sample - loss: 0.4799 - accuracy: 0.7553 - val_loss: 0.5029 - val_accuracy: 0.7266\n",
      "Epoch 101/175\n",
      "19968/20232 [============================>.] - ETA: 0s - loss: 0.4787 - accuracy: 0.7536\n",
      "Epoch 00101: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 25us/sample - loss: 0.4785 - accuracy: 0.7539 - val_loss: 0.5054 - val_accuracy: 0.7230\n",
      "Epoch 102/175\n",
      "17408/20232 [========================>.....] - ETA: 0s - loss: 0.4780 - accuracy: 0.7570\n",
      "Epoch 00102: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.4774 - accuracy: 0.7571 - val_loss: 0.5047 - val_accuracy: 0.7228\n",
      "Epoch 103/175\n",
      "20224/20232 [============================>.] - ETA: 0s - loss: 0.4777 - accuracy: 0.7546\n",
      "Epoch 00103: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 23us/sample - loss: 0.4777 - accuracy: 0.7547 - val_loss: 0.5043 - val_accuracy: 0.7212\n",
      "Epoch 104/175\n",
      "19200/20232 [===========================>..] - ETA: 0s - loss: 0.4799 - accuracy: 0.7551\n",
      "Epoch 00104: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4792 - accuracy: 0.7559 - val_loss: 0.5033 - val_accuracy: 0.7242\n",
      "Epoch 105/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4798 - accuracy: 0.7564\n",
      "Epoch 00105: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 29us/sample - loss: 0.4795 - accuracy: 0.7560 - val_loss: 0.5080 - val_accuracy: 0.7224\n",
      "Epoch 106/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4794 - accuracy: 0.7538\n",
      "Epoch 00106: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 23us/sample - loss: 0.4790 - accuracy: 0.7536 - val_loss: 0.5055 - val_accuracy: 0.7238\n",
      "Epoch 107/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4769 - accuracy: 0.7564\n",
      "Epoch 00107: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 25us/sample - loss: 0.4775 - accuracy: 0.7561 - val_loss: 0.5061 - val_accuracy: 0.7222\n",
      "Epoch 108/175\n",
      "17536/20232 [=========================>....] - ETA: 0s - loss: 0.4743 - accuracy: 0.7584\n",
      "Epoch 00108: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4760 - accuracy: 0.7565 - val_loss: 0.5093 - val_accuracy: 0.7214\n",
      "Epoch 109/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4759 - accuracy: 0.7569\n",
      "Epoch 00109: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 25us/sample - loss: 0.4763 - accuracy: 0.7575 - val_loss: 0.5056 - val_accuracy: 0.7236\n",
      "Epoch 110/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.4776 - accuracy: 0.7575\n",
      "Epoch 00110: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4771 - accuracy: 0.7575 - val_loss: 0.5095 - val_accuracy: 0.7238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/175\n",
      "19584/20232 [============================>.] - ETA: 0s - loss: 0.4775 - accuracy: 0.7585\n",
      "Epoch 00111: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 27us/sample - loss: 0.4772 - accuracy: 0.7576 - val_loss: 0.5071 - val_accuracy: 0.7224\n",
      "Epoch 112/175\n",
      "19840/20232 [============================>.] - ETA: 0s - loss: 0.4793 - accuracy: 0.7583\n",
      "Epoch 00112: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.4791 - accuracy: 0.7584 - val_loss: 0.5064 - val_accuracy: 0.7250\n",
      "Epoch 113/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4769 - accuracy: 0.7564\n",
      "Epoch 00113: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 29us/sample - loss: 0.4768 - accuracy: 0.7564 - val_loss: 0.5030 - val_accuracy: 0.7258\n",
      "Epoch 114/175\n",
      "17664/20232 [=========================>....] - ETA: 0s - loss: 0.4751 - accuracy: 0.7611\n",
      "Epoch 00114: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 22us/sample - loss: 0.4740 - accuracy: 0.7616 - val_loss: 0.5060 - val_accuracy: 0.7254\n",
      "Epoch 115/175\n",
      "19200/20232 [===========================>..] - ETA: 0s - loss: 0.4748 - accuracy: 0.7569\n",
      "Epoch 00115: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 25us/sample - loss: 0.4756 - accuracy: 0.7568 - val_loss: 0.5053 - val_accuracy: 0.7252\n",
      "Epoch 116/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4720 - accuracy: 0.7613\n",
      "Epoch 00116: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4719 - accuracy: 0.7606 - val_loss: 0.5077 - val_accuracy: 0.7240\n",
      "Epoch 117/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4752 - accuracy: 0.7573\n",
      "Epoch 00117: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 32us/sample - loss: 0.4765 - accuracy: 0.7582 - val_loss: 0.5062 - val_accuracy: 0.7236\n",
      "Epoch 118/175\n",
      "20096/20232 [============================>.] - ETA: 0s - loss: 0.4751 - accuracy: 0.7583\n",
      "Epoch 00118: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 29us/sample - loss: 0.4754 - accuracy: 0.7579 - val_loss: 0.5062 - val_accuracy: 0.7260\n",
      "Epoch 119/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4753 - accuracy: 0.7577\n",
      "Epoch 00119: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4751 - accuracy: 0.7580 - val_loss: 0.5025 - val_accuracy: 0.7264\n",
      "Epoch 120/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4777 - accuracy: 0.7551\n",
      "Epoch 00120: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 24us/sample - loss: 0.4768 - accuracy: 0.7564 - val_loss: 0.5088 - val_accuracy: 0.7210\n",
      "Epoch 121/175\n",
      "19840/20232 [============================>.] - ETA: 0s - loss: 0.4737 - accuracy: 0.7611\n",
      "Epoch 00121: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 25us/sample - loss: 0.4737 - accuracy: 0.7611 - val_loss: 0.5056 - val_accuracy: 0.7244\n",
      "Epoch 122/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.4744 - accuracy: 0.7583\n",
      "Epoch 00122: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 25us/sample - loss: 0.4751 - accuracy: 0.7573 - val_loss: 0.5070 - val_accuracy: 0.7191\n",
      "Epoch 123/175\n",
      "18432/20232 [==========================>...] - ETA: 0s - loss: 0.4725 - accuracy: 0.7570\n",
      "Epoch 00123: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 1s 33us/sample - loss: 0.4710 - accuracy: 0.7594 - val_loss: 0.5058 - val_accuracy: 0.7228\n",
      "Epoch 124/175\n",
      "19712/20232 [============================>.] - ETA: 0s - loss: 0.4740 - accuracy: 0.7598\n",
      "Epoch 00124: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.4739 - accuracy: 0.7601 - val_loss: 0.5070 - val_accuracy: 0.7218\n",
      "Epoch 125/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.4759 - accuracy: 0.7576\n",
      "Epoch 00125: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.4743 - accuracy: 0.7600 - val_loss: 0.5079 - val_accuracy: 0.7238\n",
      "Epoch 126/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.4727 - accuracy: 0.7610\n",
      "Epoch 00126: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4724 - accuracy: 0.7608 - val_loss: 0.5039 - val_accuracy: 0.7216\n",
      "Epoch 127/175\n",
      "20224/20232 [============================>.] - ETA: 0s - loss: 0.4706 - accuracy: 0.7608\n",
      "Epoch 00127: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4706 - accuracy: 0.7608 - val_loss: 0.5079 - val_accuracy: 0.7220\n",
      "Epoch 128/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4746 - accuracy: 0.75 - ETA: 0s - loss: 0.4752 - accuracy: 0.7558\n",
      "Epoch 00128: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.4753 - accuracy: 0.7553 - val_loss: 0.5066 - val_accuracy: 0.7169\n",
      "Epoch 129/175\n",
      "19968/20232 [============================>.] - ETA: 0s - loss: 0.4705 - accuracy: 0.7582\n",
      "Epoch 00129: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.4705 - accuracy: 0.7583 - val_loss: 0.5049 - val_accuracy: 0.7252\n",
      "Epoch 130/175\n",
      "20096/20232 [============================>.] - ETA: 0s - loss: 0.4729 - accuracy: 0.7587\n",
      "Epoch 00130: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 21us/sample - loss: 0.4730 - accuracy: 0.7586 - val_loss: 0.5056 - val_accuracy: 0.7193\n",
      "Epoch 131/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.4719 - accuracy: 0.7605\n",
      "Epoch 00131: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4730 - accuracy: 0.7599 - val_loss: 0.5050 - val_accuracy: 0.7240\n",
      "Epoch 132/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.4731 - accuracy: 0.7638\n",
      "Epoch 00132: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4739 - accuracy: 0.7631 - val_loss: 0.5083 - val_accuracy: 0.7179\n",
      "Epoch 133/175\n",
      "19968/20232 [============================>.] - ETA: 0s - loss: 0.4737 - accuracy: 0.7581\n",
      "Epoch 00133: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4736 - accuracy: 0.7580 - val_loss: 0.5024 - val_accuracy: 0.7228\n",
      "Epoch 134/175\n",
      "18688/20232 [==========================>...] - ETA: 0s - loss: 0.4725 - accuracy: 0.7589\n",
      "Epoch 00134: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.4738 - accuracy: 0.7580 - val_loss: 0.5015 - val_accuracy: 0.7252\n",
      "Epoch 135/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4723 - accuracy: 0.7642\n",
      "Epoch 00135: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4725 - accuracy: 0.7639 - val_loss: 0.5039 - val_accuracy: 0.7226\n",
      "Epoch 136/175\n",
      "19456/20232 [===========================>..] - ETA: 0s - loss: 0.4727 - accuracy: 0.7614\n",
      "Epoch 00136: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 22us/sample - loss: 0.4728 - accuracy: 0.7618 - val_loss: 0.5045 - val_accuracy: 0.7198\n",
      "Epoch 137/175\n",
      "17408/20232 [========================>.....] - ETA: 0s - loss: 0.4708 - accuracy: 0.7594\n",
      "Epoch 00137: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 20us/sample - loss: 0.4728 - accuracy: 0.7587 - val_loss: 0.5070 - val_accuracy: 0.7202\n",
      "Epoch 138/175\n",
      "18688/20232 [==========================>...] - ETA: 0s - loss: 0.4691 - accuracy: 0.7605\n",
      "Epoch 00138: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4695 - accuracy: 0.7605 - val_loss: 0.5046 - val_accuracy: 0.7218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/175\n",
      "18560/20232 [==========================>...] - ETA: 0s - loss: 0.4715 - accuracy: 0.7645\n",
      "Epoch 00139: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4715 - accuracy: 0.7645 - val_loss: 0.5054 - val_accuracy: 0.7236\n",
      "Epoch 140/175\n",
      "19200/20232 [===========================>..] - ETA: 0s - loss: 0.4720 - accuracy: 0.7629\n",
      "Epoch 00140: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4715 - accuracy: 0.7640 - val_loss: 0.5070 - val_accuracy: 0.7222\n",
      "Epoch 141/175\n",
      "19328/20232 [===========================>..] - ETA: 0s - loss: 0.4720 - accuracy: 0.7596\n",
      "Epoch 00141: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4722 - accuracy: 0.7589 - val_loss: 0.5062 - val_accuracy: 0.7210\n",
      "Epoch 142/175\n",
      "20096/20232 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7659\n",
      "Epoch 00142: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4688 - accuracy: 0.7661 - val_loss: 0.5073 - val_accuracy: 0.7193\n",
      "Epoch 143/175\n",
      "19584/20232 [============================>.] - ETA: 0s - loss: 0.4714 - accuracy: 0.7608\n",
      "Epoch 00143: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4707 - accuracy: 0.7609 - val_loss: 0.5063 - val_accuracy: 0.7244\n",
      "Epoch 144/175\n",
      "17664/20232 [=========================>....] - ETA: 0s - loss: 0.4748 - accuracy: 0.7579\n",
      "Epoch 00144: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4741 - accuracy: 0.7586 - val_loss: 0.5034 - val_accuracy: 0.7230\n",
      "Epoch 145/175\n",
      "17664/20232 [=========================>....] - ETA: 0s - loss: 0.4649 - accuracy: 0.7644\n",
      "Epoch 00145: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4650 - accuracy: 0.7657 - val_loss: 0.5071 - val_accuracy: 0.7214\n",
      "Epoch 146/175\n",
      "17920/20232 [=========================>....] - ETA: 0s - loss: 0.4692 - accuracy: 0.7641\n",
      "Epoch 00146: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4691 - accuracy: 0.7641 - val_loss: 0.5074 - val_accuracy: 0.7240\n",
      "Epoch 147/175\n",
      "18432/20232 [==========================>...] - ETA: 0s - loss: 0.4683 - accuracy: 0.7630\n",
      "Epoch 00147: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4701 - accuracy: 0.7620 - val_loss: 0.5053 - val_accuracy: 0.7242\n",
      "Epoch 148/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4669 - accuracy: 0.7617\n",
      "Epoch 00148: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4680 - accuracy: 0.7617 - val_loss: 0.5085 - val_accuracy: 0.7212\n",
      "Epoch 149/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4708 - accuracy: 0.7602\n",
      "Epoch 00149: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4723 - accuracy: 0.7604 - val_loss: 0.5068 - val_accuracy: 0.7248\n",
      "Epoch 150/175\n",
      "19840/20232 [============================>.] - ETA: 0s - loss: 0.4700 - accuracy: 0.7567\n",
      "Epoch 00150: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4691 - accuracy: 0.7578 - val_loss: 0.5074 - val_accuracy: 0.7258\n",
      "Epoch 151/175\n",
      "19328/20232 [===========================>..] - ETA: 0s - loss: 0.4684 - accuracy: 0.7621\n",
      "Epoch 00151: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4688 - accuracy: 0.7618 - val_loss: 0.5049 - val_accuracy: 0.7254\n",
      "Epoch 152/175\n",
      "18432/20232 [==========================>...] - ETA: 0s - loss: 0.4652 - accuracy: 0.7639\n",
      "Epoch 00152: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4651 - accuracy: 0.7636 - val_loss: 0.5061 - val_accuracy: 0.7197\n",
      "Epoch 153/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4691 - accuracy: 0.7598\n",
      "Epoch 00153: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4687 - accuracy: 0.7602 - val_loss: 0.5073 - val_accuracy: 0.7183\n",
      "Epoch 154/175\n",
      "18176/20232 [=========================>....] - ETA: 0s - loss: 0.4693 - accuracy: 0.7663\n",
      "Epoch 00154: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4703 - accuracy: 0.7653 - val_loss: 0.5074 - val_accuracy: 0.7147\n",
      "Epoch 155/175\n",
      "18688/20232 [==========================>...] - ETA: 0s - loss: 0.4673 - accuracy: 0.7665\n",
      "Epoch 00155: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4690 - accuracy: 0.7646 - val_loss: 0.5053 - val_accuracy: 0.7206\n",
      "Epoch 156/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4686 - accuracy: 0.7585\n",
      "Epoch 00156: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4689 - accuracy: 0.7577 - val_loss: 0.5087 - val_accuracy: 0.7187\n",
      "Epoch 157/175\n",
      "19328/20232 [===========================>..] - ETA: 0s - loss: 0.4674 - accuracy: 0.7621\n",
      "Epoch 00157: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4670 - accuracy: 0.7618 - val_loss: 0.5043 - val_accuracy: 0.7252\n",
      "Epoch 158/175\n",
      "17792/20232 [=========================>....] - ETA: 0s - loss: 0.4653 - accuracy: 0.7648\n",
      "Epoch 00158: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4664 - accuracy: 0.7625 - val_loss: 0.5040 - val_accuracy: 0.7252\n",
      "Epoch 159/175\n",
      "18560/20232 [==========================>...] - ETA: 0s - loss: 0.4649 - accuracy: 0.7650\n",
      "Epoch 00159: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4650 - accuracy: 0.7648 - val_loss: 0.5053 - val_accuracy: 0.7246\n",
      "Epoch 160/175\n",
      "19072/20232 [===========================>..] - ETA: 0s - loss: 0.4676 - accuracy: 0.7658\n",
      "Epoch 00160: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4669 - accuracy: 0.7669 - val_loss: 0.5094 - val_accuracy: 0.7256\n",
      "Epoch 161/175\n",
      "17792/20232 [=========================>....] - ETA: 0s - loss: 0.4664 - accuracy: 0.7637\n",
      "Epoch 00161: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4662 - accuracy: 0.7628 - val_loss: 0.5082 - val_accuracy: 0.7268\n",
      "Epoch 162/175\n",
      "17536/20232 [=========================>....] - ETA: 0s - loss: 0.4664 - accuracy: 0.7629\n",
      "Epoch 00162: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4672 - accuracy: 0.7625 - val_loss: 0.5037 - val_accuracy: 0.7280\n",
      "Epoch 163/175\n",
      "17536/20232 [=========================>....] - ETA: 0s - loss: 0.4630 - accuracy: 0.7701\n",
      "Epoch 00163: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4632 - accuracy: 0.7691 - val_loss: 0.5077 - val_accuracy: 0.7216\n",
      "Epoch 164/175\n",
      "19968/20232 [============================>.] - ETA: 0s - loss: 0.4662 - accuracy: 0.7651\n",
      "Epoch 00164: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4664 - accuracy: 0.7652 - val_loss: 0.5067 - val_accuracy: 0.7268\n",
      "Epoch 165/175\n",
      "19328/20232 [===========================>..] - ETA: 0s - loss: 0.4654 - accuracy: 0.7630\n",
      "Epoch 00165: val_accuracy did not improve from 0.72855\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4661 - accuracy: 0.7630 - val_loss: 0.5054 - val_accuracy: 0.7254\n",
      "Epoch 166/175\n",
      "18816/20232 [==========================>...] - ETA: 0s - loss: 0.4652 - accuracy: 0.7630\n",
      "Epoch 00166: val_accuracy improved from 0.72855 to 0.72993, saving model to data/best_model.h5\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4668 - accuracy: 0.7621 - val_loss: 0.5066 - val_accuracy: 0.7299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/175\n",
      "18304/20232 [==========================>...] - ETA: 0s - loss: 0.4642 - accuracy: 0.7666\n",
      "Epoch 00167: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4636 - accuracy: 0.7663 - val_loss: 0.5063 - val_accuracy: 0.7266\n",
      "Epoch 168/175\n",
      "17024/20232 [========================>.....] - ETA: 0s - loss: 0.4680 - accuracy: 0.7659\n",
      "Epoch 00168: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4692 - accuracy: 0.7642 - val_loss: 0.5052 - val_accuracy: 0.7268\n",
      "Epoch 169/175\n",
      "18944/20232 [===========================>..] - ETA: 0s - loss: 0.4664 - accuracy: 0.7658\n",
      "Epoch 00169: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 19us/sample - loss: 0.4661 - accuracy: 0.7648 - val_loss: 0.5062 - val_accuracy: 0.7242\n",
      "Epoch 170/175\n",
      "19328/20232 [===========================>..] - ETA: 0s - loss: 0.4657 - accuracy: 0.7647\n",
      "Epoch 00170: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 18us/sample - loss: 0.4648 - accuracy: 0.7651 - val_loss: 0.5069 - val_accuracy: 0.7220\n",
      "Epoch 171/175\n",
      "17408/20232 [========================>.....] - ETA: 0s - loss: 0.4627 - accuracy: 0.7649\n",
      "Epoch 00171: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4648 - accuracy: 0.7639 - val_loss: 0.5047 - val_accuracy: 0.7218\n",
      "Epoch 172/175\n",
      "19200/20232 [===========================>..] - ETA: 0s - loss: 0.4703 - accuracy: 0.7596\n",
      "Epoch 00172: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 16us/sample - loss: 0.4708 - accuracy: 0.7597 - val_loss: 0.5042 - val_accuracy: 0.7210\n",
      "Epoch 173/175\n",
      "19712/20232 [============================>.] - ETA: 0s - loss: 0.4688 - accuracy: 0.7609\n",
      "Epoch 00173: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4682 - accuracy: 0.7609 - val_loss: 0.5094 - val_accuracy: 0.7195\n",
      "Epoch 174/175\n",
      "16128/20232 [======================>.......] - ETA: 0s - loss: 0.4657 - accuracy: 0.7647\n",
      "Epoch 00174: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 15us/sample - loss: 0.4667 - accuracy: 0.7651 - val_loss: 0.5083 - val_accuracy: 0.7202\n",
      "Epoch 175/175\n",
      "17792/20232 [=========================>....] - ETA: 0s - loss: 0.4643 - accuracy: 0.7646\n",
      "Epoch 00175: val_accuracy did not improve from 0.72993\n",
      "20232/20232 [==============================] - 0s 17us/sample - loss: 0.4670 - accuracy: 0.7633 - val_loss: 0.5075 - val_accuracy: 0.7204\n"
     ]
    }
   ],
   "source": [
    "hist = m.fit( Z_train,\n",
    "                    y_train, \n",
    "                    batch_size=128,\n",
    "                    validation_data=(Z_test, y_test),\n",
    "                    epochs=175,\n",
    "                    callbacks=[mc],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(hist.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.plot(hist.history['accuracy'], label='Train Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
