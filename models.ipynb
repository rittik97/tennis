{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Model to predict match results](#first-bullet)\n",
    "* [Model to predict # of games](#second-bullet)\n",
    "* [test results](#third-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from sklearn.metrics import accuracy_score,r2_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/final_2.csv' ,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ATP</th>\n",
       "      <th>B365L</th>\n",
       "      <th>B365W</th>\n",
       "      <th>Best of</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Court</th>\n",
       "      <th>Date</th>\n",
       "      <th>L1</th>\n",
       "      <th>...</th>\n",
       "      <th>one_clay_vv</th>\n",
       "      <th>one_grass_vv</th>\n",
       "      <th>one_hard_vv</th>\n",
       "      <th>one_three_vv</th>\n",
       "      <th>one_five_vv</th>\n",
       "      <th>pts_diff</th>\n",
       "      <th>player_one_pts</th>\n",
       "      <th>player_two_pts</th>\n",
       "      <th>player_one_sets</th>\n",
       "      <th>player_two_sets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000569ca-bcdb-4266-98a0-b409cb8932ff</td>\n",
       "      <td>383</td>\n",
       "      <td>11</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>2016-02-09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00056f39-ba9b-4807-ac20-cbbcf9e2c304</td>\n",
       "      <td>1887</td>\n",
       "      <td>45</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3</td>\n",
       "      <td>Retired</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>2010-08-04</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-610.0</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a7190-03aa-44b3-b631-e85d00f265ea</td>\n",
       "      <td>1537</td>\n",
       "      <td>39</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.36</td>\n",
       "      <td>5</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>895.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c3ad6-ac2d-4ca8-abe9-853099f8a568</td>\n",
       "      <td>597</td>\n",
       "      <td>18</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>2012-02-28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6663.0</td>\n",
       "      <td>7150.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014c43e-cdee-41e7-a5f8-60a15a6f9cc2</td>\n",
       "      <td>2510</td>\n",
       "      <td>63</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.07</td>\n",
       "      <td>3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5311.0</td>\n",
       "      <td>5820.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  Unnamed: 0  ATP  B365L  B365W  \\\n",
       "0  000569ca-bcdb-4266-98a0-b409cb8932ff         383   11   5.00   1.16   \n",
       "1  00056f39-ba9b-4807-ac20-cbbcf9e2c304        1887   45   1.36   3.00   \n",
       "2  000a7190-03aa-44b3-b631-e85d00f265ea        1537   39   3.00   1.36   \n",
       "3  000c3ad6-ac2d-4ca8-abe9-853099f8a568         597   18  19.00   1.02   \n",
       "4  0014c43e-cdee-41e7-a5f8-60a15a6f9cc2        2510   63   9.00   1.07   \n",
       "\n",
       "   Best of    Comment    Court        Date   L1  ...  one_clay_vv  \\\n",
       "0        3  Completed   Indoor  2016-02-09  3.0  ...          0.0   \n",
       "1        3    Retired  Outdoor  2010-08-04  5.0  ...          NaN   \n",
       "2        5  Completed  Outdoor  2015-06-29  3.0  ...          0.0   \n",
       "3        3  Completed  Outdoor  2012-02-28  3.0  ...          1.0   \n",
       "4        3  Completed   Indoor  2016-10-25  7.0  ...          0.0   \n",
       "\n",
       "   one_grass_vv  one_hard_vv  one_three_vv  one_five_vv  pts_diff  \\\n",
       "0           0.0          1.0           1.0          0.0     301.0   \n",
       "1           NaN          NaN           NaN          NaN    -610.0   \n",
       "2           1.0          1.0           1.0          1.0     347.0   \n",
       "3           0.0          1.0           1.0          1.0    6663.0   \n",
       "4           0.0          1.0           1.0          0.0    5311.0   \n",
       "\n",
       "  player_one_pts player_two_pts  player_one_sets player_two_sets  \n",
       "0          414.0          113.0              2.0             0.0  \n",
       "1         1385.0          775.0              0.0             1.0  \n",
       "2          895.0          548.0              3.0             2.0  \n",
       "3         7150.0          487.0              2.0             1.0  \n",
       "4         5820.0          509.0              2.0             1.0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comment\n",
       "Awarded         0.000035\n",
       "Completed       0.961710\n",
       "Disqualified    0.000035\n",
       "Retired         0.031937\n",
       "Sched           0.000069\n",
       "Walkover        0.006215\n",
       "Name: Comment, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Comment')['Comment'].count()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data.Comment=='Completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of games with only top 100 players:  0.6574639190062469\n"
     ]
    }
   ],
   "source": [
    "print(\"% of games with only top 100 players: \",len(data[(data.WRank<=100) & (data.LRank<=100)])/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome\n",
       "0    0.335715\n",
       "1    0.664285\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of times the higher ranked player one\n",
    "data.groupby('outcome')['outcome'].count()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of times the higher ranked player one in 5set games\n",
    "data[data['Best of']==5].groupby('outcome')['outcome'].count()/len(data[data['Best of']==5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of times the higher ranked player one in 5set games\n",
    "data[data['Best of']==3].groupby('total_sets')['total_sets'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['Best of']==3) & (data.total_sets==1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rank_dif']=data['player_two_rank']-data['player_one_rank']\n",
    "data['pts_dif']=data['player_two_pts']-data['player_one_pts']\n",
    "data['win_rate_diff']=data['two_win_rate_year']-data['one_win_rate_year']\n",
    "data['hard_diff']=data['two_hard_year']-data['one_hard_year']\n",
    "data['five_diff']=data['two_five_year']-data['one_five_year']\n",
    "data['three_diff']=data['two_three_year']-data['one_three_year']\n",
    "\n",
    "data['hard_vv']=data['two_hard_vv']-data['one_hard_vv']\n",
    "data['vv']=data['two_win_rate_vv']-data['one_win_rate_vv']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model to predict match results <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uuid', 'Unnamed: 0', 'ATP', 'B365L', 'B365W', 'Best of', 'Comment',\n",
       "       'Court', 'Date', 'L1', 'L2', 'L3', 'L4', 'L5', 'LPts', 'LRank',\n",
       "       'Location', 'Loser', 'Lsets', 'Round', 'Series', 'Surface',\n",
       "       'Tournament', 'W1', 'W2', 'W3', 'W4', 'W5', 'WPts', 'WRank', 'Winner',\n",
       "       'Wsets', 'rank_diff', 'outcome', 'player_one', 'player_two',\n",
       "       'player_one_rank', 'player_two_rank', 'uuid.1', 'one_name', 'one_date',\n",
       "       'one_cutoff_date', 'one_win_rate_year', 'one_games_played_year',\n",
       "       'one_clay_year', 'one_grass_year', 'one_hard_year', 'one_three_year',\n",
       "       'one_five_year', 'two_name', 'two_date', 'two_cutoff_date',\n",
       "       'two_win_rate_year', 'two_games_played_year', 'two_clay_year',\n",
       "       'two_grass_year', 'two_hard_year', 'two_three_year', 'two_five_year',\n",
       "       'major', 'total_games', 'player_one_total_games',\n",
       "       'player_two_total_games', 'total_sets', 'player_two_name_vv',\n",
       "       'two_cutoff_date_vv', 'two_win_rate_vv', 'two_games_played_vv',\n",
       "       'two_clay_vv', 'two_grass_vv', 'two_hard_vv', 'two_three_vv',\n",
       "       'two_five_vv', 'player_one_name_vv', 'one_cutoff_date_vv',\n",
       "       'one_win_rate_vv', 'one_games_played_vv', 'one_clay_vv', 'one_grass_vv',\n",
       "       'one_hard_vv', 'one_three_vv', 'one_five_vv', 'pts_diff',\n",
       "       'player_one_pts', 'player_two_pts', 'player_one_sets',\n",
       "       'player_two_sets', 'rank_dif', 'pts_dif', 'win_rate_diff', 'hard_diff',\n",
       "       'five_diff', 'three_diff', 'hard_vv', 'vv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27854, 95)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_columns=['player_one_rank','player_two_rank','Surface', 'Best of','one_win_rate_year',\n",
    "       'one_games_played_year', 'one_clay_year', 'one_grass_year',\n",
    "       'one_hard_year', 'one_three_year', 'one_five_year', 'two_win_rate_year',\n",
    "       'two_games_played_year', 'two_clay_year', 'two_grass_year',\n",
    "       'two_hard_year', 'two_three_year', 'two_five_year', 'major','one_win_rate_vv', 'one_games_played_vv', 'one_clay_vv', 'one_grass_vv',\n",
    "       'one_hard_vv', 'one_three_vv', 'one_five_vv','two_win_rate_vv',\n",
    "       'two_games_played_vv', 'two_clay_vv', 'two_grass_vv', 'two_hard_vv',\n",
    "       'two_three_vv', 'two_five_vv', 'rank_dif', 'win_rate_diff', 'hard_diff',\n",
    "       'five_diff', 'three_diff', 'hard_vv', 'vv' ,'pts_dif', 'outcome']\n",
    "data=data[feature_columns].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='outcome'\n",
    "y=data[target]\n",
    "X=data.drop('outcome',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    (['player_one_rank'], [StandardScaler(),SimpleImputer()]),\n",
    "    (['player_two_rank'], [StandardScaler(),SimpleImputer()]),\n",
    "    (['Surface'], [SimpleImputer(strategy='constant', fill_value='most_frequent'),LabelBinarizer()]),\n",
    "    ('Best of', LabelEncoder()),\n",
    "    (['one_win_rate_year'],StandardScaler()),\n",
    "    (['one_games_played_year'], StandardScaler()),\n",
    "    (['one_clay_year'], StandardScaler()),\n",
    "    (['one_grass_year'],StandardScaler()),\n",
    "    (['one_hard_year'], StandardScaler()),\n",
    "    (['one_three_year'], StandardScaler()),\n",
    "    (['one_five_year'], StandardScaler()),\n",
    "    (['two_win_rate_year'],StandardScaler()),\n",
    "    (['two_games_played_year'], StandardScaler()),\n",
    "    (['two_clay_year'], StandardScaler()),\n",
    "    (['two_grass_year'],StandardScaler()),\n",
    "    (['two_hard_year'], StandardScaler()),\n",
    "    (['two_three_year'], StandardScaler()),\n",
    "    (['two_five_year'], StandardScaler()),\n",
    "    ('major', LabelEncoder()),\n",
    "    (['one_win_rate_vv'],StandardScaler()), \n",
    "    (['one_games_played_vv'],StandardScaler()),  \n",
    "    (['one_clay_vv'],StandardScaler()),  \n",
    "    (['one_grass_vv'],StandardScaler()), \n",
    "    (['one_hard_vv'],StandardScaler()),  \n",
    "    (['one_three_vv'],StandardScaler()),  \n",
    "    (['one_five_vv'],StandardScaler()),\n",
    "    (['two_win_rate_vv'],StandardScaler()), \n",
    "    (['two_games_played_vv'],StandardScaler()),  \n",
    "    (['two_clay_vv'],StandardScaler()),  \n",
    "    (['two_grass_vv'],StandardScaler()), \n",
    "    (['two_hard_vv'],StandardScaler()),  \n",
    "    (['two_three_vv'],StandardScaler()),  \n",
    "    (['two_five_vv'],StandardScaler()),    \n",
    "    (['rank_dif'],StandardScaler()),  \n",
    "    (['pts_dif'],[StandardScaler(),SimpleImputer()]),  \n",
    "    (['win_rate_diff'],StandardScaler()),  \n",
    "    (['hard_diff'],StandardScaler()),\n",
    "    (['five_diff'],StandardScaler()), \n",
    "    (['three_diff'],StandardScaler()),\n",
    "    (['hard_vv'],StandardScaler()),\n",
    "    (['vv'],StandardScaler())\n",
    "    \n",
    "], df_out=\n",
    "    True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 215 ms, sys: 32.5 ms, total: 248 ms\n",
      "Wall time: 255 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Z_train=mapper.fit_transform(X_train)\n",
    "Z_test=mapper.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(C=20, max_iter=1000, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=20, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Z_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6944223107569721"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(Z_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7038438558056164, 0.623274026470725)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred),roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([['major', 'three_diff', 'two_hard_year', 'hard_diff', 'hard_vv', 'two_five_vv', 'two_grass_year', 'two_clay_vv', 'two_grass_vv', 'five_diff', 'two_hard_vv', 'one_clay_year', 'two_five_year', 'player_one_rank', 'two_three_vv', 'one_hard_year', 'pts_dif', 'vv', 'one_games_played_vv', 'two_games_played_vv', 'two_clay_year', 'player_two_rank', 'one_five_year', 'rank_dif', 'one_grass_year', 'one_games_played_year', 'two_games_played_year', 'one_grass_vv', 'two_three_year', 'win_rate_diff', 'one_three_vv', 'one_hard_vv', 'Surface_Hard', 'one_clay_vv', 'one_five_vv', 'Surface_Grass', 'one_win_rate_year', 'Surface_Clay', 'one_three_year', 'two_win_rate_year', 'Best of', 'two_win_rate_vv', 'one_win_rate_vv']], dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_train.columns[model.coef_.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(units=64, activation='relu', input_shape=(Z_train.shape[1],)))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Dense(units=32, activation='relu'))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Dense(units=16, activation='relu'))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Dense(units=5, activation='relu'))\n",
    "m.add(Dropout(0.2))\n",
    "m.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "mc=ModelCheckpoint('data/best_model.h5', monitor='val_accuracy', mode='max', verbose=2, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20080 samples, validate on 5021 samples\n",
      "Epoch 1/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.5979 - accuracy: 0.6634\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.69030, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 1s 48us/sample - loss: 0.5929 - accuracy: 0.6654 - val_loss: 0.5429 - val_accuracy: 0.6903\n",
      "Epoch 2/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.5614 - accuracy: 0.6820\n",
      "Epoch 00002: val_accuracy improved from 0.69030 to 0.70962, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.5621 - accuracy: 0.6800 - val_loss: 0.5365 - val_accuracy: 0.7096\n",
      "Epoch 3/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.5555 - accuracy: 0.6854\n",
      "Epoch 00003: val_accuracy did not improve from 0.70962\n",
      "20080/20080 [==============================] - 0s 20us/sample - loss: 0.5548 - accuracy: 0.6863 - val_loss: 0.5296 - val_accuracy: 0.7076\n",
      "Epoch 4/175\n",
      "19456/20080 [============================>.] - ETA: 0s - loss: 0.5466 - accuracy: 0.6924\n",
      "Epoch 00004: val_accuracy improved from 0.70962 to 0.71480, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.5462 - accuracy: 0.6924 - val_loss: 0.5272 - val_accuracy: 0.7148\n",
      "Epoch 5/175\n",
      "16000/20080 [======================>.......] - ETA: 0s - loss: 0.5434 - accuracy: 0.6954\n",
      "Epoch 00005: val_accuracy did not improve from 0.71480\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.5409 - accuracy: 0.6968 - val_loss: 0.5234 - val_accuracy: 0.7134\n",
      "Epoch 6/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.5402 - accuracy: 0.6964\n",
      "Epoch 00006: val_accuracy did not improve from 0.71480\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.5424 - accuracy: 0.6950 - val_loss: 0.5257 - val_accuracy: 0.7140\n",
      "Epoch 7/175\n",
      "18816/20080 [===========================>..] - ETA: 0s - loss: 0.5379 - accuracy: 0.7002\n",
      "Epoch 00007: val_accuracy did not improve from 0.71480\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.5373 - accuracy: 0.7008 - val_loss: 0.5225 - val_accuracy: 0.7098\n",
      "Epoch 8/175\n",
      "19328/20080 [===========================>..] - ETA: 0s - loss: 0.5347 - accuracy: 0.7016\n",
      "Epoch 00008: val_accuracy did not improve from 0.71480\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.5342 - accuracy: 0.7013 - val_loss: 0.5201 - val_accuracy: 0.7140\n",
      "Epoch 9/175\n",
      "19840/20080 [============================>.] - ETA: 0s - loss: 0.5327 - accuracy: 0.7043\n",
      "Epoch 00009: val_accuracy improved from 0.71480 to 0.72157, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.5329 - accuracy: 0.7044 - val_loss: 0.5196 - val_accuracy: 0.7216\n",
      "Epoch 10/175\n",
      "19584/20080 [============================>.] - ETA: 0s - loss: 0.5316 - accuracy: 0.7089\n",
      "Epoch 00010: val_accuracy did not improve from 0.72157\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.5322 - accuracy: 0.7080 - val_loss: 0.5167 - val_accuracy: 0.7186\n",
      "Epoch 11/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.5303 - accuracy: 0.7096\n",
      "Epoch 00011: val_accuracy improved from 0.72157 to 0.72197, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 21us/sample - loss: 0.5293 - accuracy: 0.7097 - val_loss: 0.5142 - val_accuracy: 0.7220\n",
      "Epoch 12/175\n",
      "19200/20080 [===========================>..] - ETA: 0s - loss: 0.5268 - accuracy: 0.7107\n",
      "Epoch 00012: val_accuracy did not improve from 0.72197\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.5284 - accuracy: 0.7097 - val_loss: 0.5173 - val_accuracy: 0.7194\n",
      "Epoch 13/175\n",
      "19072/20080 [===========================>..] - ETA: 0s - loss: 0.5235 - accuracy: 0.7170\n",
      "Epoch 00013: val_accuracy improved from 0.72197 to 0.72237, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 20us/sample - loss: 0.5239 - accuracy: 0.7168 - val_loss: 0.5134 - val_accuracy: 0.7224\n",
      "Epoch 14/175\n",
      "18176/20080 [==========================>...] - ETA: 0s - loss: 0.5228 - accuracy: 0.7174\n",
      "Epoch 00014: val_accuracy did not improve from 0.72237\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.5230 - accuracy: 0.7169 - val_loss: 0.5149 - val_accuracy: 0.7216\n",
      "Epoch 15/175\n",
      "18816/20080 [===========================>..] - ETA: 0s - loss: 0.5218 - accuracy: 0.7146\n",
      "Epoch 00015: val_accuracy improved from 0.72237 to 0.72575, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 1s 26us/sample - loss: 0.5216 - accuracy: 0.7151 - val_loss: 0.5123 - val_accuracy: 0.7258\n",
      "Epoch 16/175\n",
      "19328/20080 [===========================>..] - ETA: 0s - loss: 0.5220 - accuracy: 0.7203\n",
      "Epoch 00016: val_accuracy did not improve from 0.72575\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.5224 - accuracy: 0.7195 - val_loss: 0.5117 - val_accuracy: 0.7232\n",
      "Epoch 17/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.5185 - accuracy: 0.7156\n",
      "Epoch 00017: val_accuracy did not improve from 0.72575\n",
      "20080/20080 [==============================] - 0s 20us/sample - loss: 0.5198 - accuracy: 0.7157 - val_loss: 0.5110 - val_accuracy: 0.7242\n",
      "Epoch 18/175\n",
      "18432/20080 [==========================>...] - ETA: 0s - loss: 0.5167 - accuracy: 0.7200\n",
      "Epoch 00018: val_accuracy did not improve from 0.72575\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.5178 - accuracy: 0.7200 - val_loss: 0.5107 - val_accuracy: 0.7258\n",
      "Epoch 19/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.5182 - accuracy: 0.7174\n",
      "Epoch 00019: val_accuracy did not improve from 0.72575\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.5168 - accuracy: 0.7194 - val_loss: 0.5093 - val_accuracy: 0.7234\n",
      "Epoch 20/175\n",
      "18176/20080 [==========================>...] - ETA: 0s - loss: 0.5156 - accuracy: 0.7197\n",
      "Epoch 00020: val_accuracy did not improve from 0.72575\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.5157 - accuracy: 0.7191 - val_loss: 0.5097 - val_accuracy: 0.7236\n",
      "Epoch 21/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.5152 - accuracy: 0.7232\n",
      "Epoch 00021: val_accuracy did not improve from 0.72575\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.5160 - accuracy: 0.7216 - val_loss: 0.5101 - val_accuracy: 0.7234\n",
      "Epoch 22/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.5130 - accuracy: 0.7227\n",
      "Epoch 00022: val_accuracy improved from 0.72575 to 0.72754, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 18us/sample - loss: 0.5152 - accuracy: 0.7200 - val_loss: 0.5083 - val_accuracy: 0.7275\n",
      "Epoch 23/175\n",
      "19712/20080 [============================>.] - ETA: 0s - loss: 0.5149 - accuracy: 0.7229\n",
      "Epoch 00023: val_accuracy improved from 0.72754 to 0.72894, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.5146 - accuracy: 0.7227 - val_loss: 0.5079 - val_accuracy: 0.7289\n",
      "Epoch 24/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.5089 - accuracy: 0.7252\n",
      "Epoch 00024: val_accuracy improved from 0.72894 to 0.73073, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.5092 - accuracy: 0.7263 - val_loss: 0.5075 - val_accuracy: 0.7307\n",
      "Epoch 25/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.5113 - accuracy: 0.7268\n",
      "Epoch 00025: val_accuracy did not improve from 0.73073\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5101 - accuracy: 0.7273 - val_loss: 0.5088 - val_accuracy: 0.7256\n",
      "Epoch 26/175\n",
      "19456/20080 [============================>.] - ETA: 0s - loss: 0.5092 - accuracy: 0.7275\n",
      "Epoch 00026: val_accuracy did not improve from 0.73073\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.5099 - accuracy: 0.7273 - val_loss: 0.5074 - val_accuracy: 0.7269\n",
      "Epoch 27/175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/20080 [============================>.] - ETA: 0s - loss: 0.5088 - accuracy: 0.7274\n",
      "Epoch 00027: val_accuracy improved from 0.73073 to 0.73173, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.5088 - accuracy: 0.7276 - val_loss: 0.5083 - val_accuracy: 0.7317\n",
      "Epoch 28/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.5060 - accuracy: 0.7311\n",
      "Epoch 00028: val_accuracy improved from 0.73173 to 0.73432, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.5074 - accuracy: 0.7286 - val_loss: 0.5060 - val_accuracy: 0.7343\n",
      "Epoch 29/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.5063 - accuracy: 0.7279\n",
      "Epoch 00029: val_accuracy did not improve from 0.73432\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5068 - accuracy: 0.7266 - val_loss: 0.5045 - val_accuracy: 0.7343\n",
      "Epoch 30/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.5049 - accuracy: 0.7304\n",
      "Epoch 00030: val_accuracy did not improve from 0.73432\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5038 - accuracy: 0.7322 - val_loss: 0.5057 - val_accuracy: 0.7289\n",
      "Epoch 31/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.5078 - accuracy: 0.7288\n",
      "Epoch 00031: val_accuracy did not improve from 0.73432\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5083 - accuracy: 0.7277 - val_loss: 0.5067 - val_accuracy: 0.7321\n",
      "Epoch 32/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.5079 - accuracy: 0.7296\n",
      "Epoch 00032: val_accuracy did not improve from 0.73432\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5070 - accuracy: 0.7291 - val_loss: 0.5039 - val_accuracy: 0.7295\n",
      "Epoch 33/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.5027 - accuracy: 0.7343\n",
      "Epoch 00033: val_accuracy did not improve from 0.73432\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5047 - accuracy: 0.7322 - val_loss: 0.5049 - val_accuracy: 0.7303\n",
      "Epoch 34/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.5061 - accuracy: 0.7309\n",
      "Epoch 00034: val_accuracy did not improve from 0.73432\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5053 - accuracy: 0.7326 - val_loss: 0.5041 - val_accuracy: 0.7297\n",
      "Epoch 35/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.5052 - accuracy: 0.7303\n",
      "Epoch 00035: val_accuracy improved from 0.73432 to 0.73591, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.5048 - accuracy: 0.7312 - val_loss: 0.5027 - val_accuracy: 0.7359\n",
      "Epoch 36/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.5008 - accuracy: 0.7362\n",
      "Epoch 00036: val_accuracy did not improve from 0.73591\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5033 - accuracy: 0.7344 - val_loss: 0.5038 - val_accuracy: 0.7307\n",
      "Epoch 37/175\n",
      "16896/20080 [========================>.....] - ETA: 0s - loss: 0.5026 - accuracy: 0.7321\n",
      "Epoch 00037: val_accuracy did not improve from 0.73591\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5037 - accuracy: 0.7329 - val_loss: 0.5031 - val_accuracy: 0.7341\n",
      "Epoch 38/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.4981 - accuracy: 0.7350\n",
      "Epoch 00038: val_accuracy improved from 0.73591 to 0.73790, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.5002 - accuracy: 0.7338 - val_loss: 0.5020 - val_accuracy: 0.7379\n",
      "Epoch 39/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.5020 - accuracy: 0.7327\n",
      "Epoch 00039: val_accuracy did not improve from 0.73790\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.5025 - accuracy: 0.7314 - val_loss: 0.5005 - val_accuracy: 0.7371\n",
      "Epoch 40/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4993 - accuracy: 0.7352\n",
      "Epoch 00040: val_accuracy did not improve from 0.73790\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4992 - accuracy: 0.7354 - val_loss: 0.4986 - val_accuracy: 0.7371\n",
      "Epoch 41/175\n",
      "17792/20080 [=========================>....] - ETA: 0s - loss: 0.4987 - accuracy: 0.7385\n",
      "Epoch 00041: val_accuracy did not improve from 0.73790\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4990 - accuracy: 0.7377 - val_loss: 0.5012 - val_accuracy: 0.7337\n",
      "Epoch 42/175\n",
      "17792/20080 [=========================>....] - ETA: 0s - loss: 0.4962 - accuracy: 0.7377\n",
      "Epoch 00042: val_accuracy improved from 0.73790 to 0.73949, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4970 - accuracy: 0.7368 - val_loss: 0.5006 - val_accuracy: 0.7395\n",
      "Epoch 43/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.5012 - accuracy: 0.7346\n",
      "Epoch 00043: val_accuracy did not improve from 0.73949\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4979 - accuracy: 0.7357 - val_loss: 0.5010 - val_accuracy: 0.7329\n",
      "Epoch 44/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4952 - accuracy: 0.7392\n",
      "Epoch 00044: val_accuracy did not improve from 0.73949\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4965 - accuracy: 0.7390 - val_loss: 0.4986 - val_accuracy: 0.7349\n",
      "Epoch 45/175\n",
      "17664/20080 [=========================>....] - ETA: 0s - loss: 0.4967 - accuracy: 0.7384\n",
      "Epoch 00045: val_accuracy did not improve from 0.73949\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4972 - accuracy: 0.7383 - val_loss: 0.5004 - val_accuracy: 0.7345\n",
      "Epoch 46/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.4973 - accuracy: 0.7420\n",
      "Epoch 00046: val_accuracy did not improve from 0.73949\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4963 - accuracy: 0.7409 - val_loss: 0.4987 - val_accuracy: 0.7305\n",
      "Epoch 47/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.4936 - accuracy: 0.7398\n",
      "Epoch 00047: val_accuracy improved from 0.73949 to 0.73969, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4928 - accuracy: 0.7393 - val_loss: 0.4990 - val_accuracy: 0.7397\n",
      "Epoch 48/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.4976 - accuracy: 0.7398\n",
      "Epoch 00048: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4976 - accuracy: 0.7393 - val_loss: 0.4977 - val_accuracy: 0.7351\n",
      "Epoch 49/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.4924 - accuracy: 0.7403 ETA: 0s - loss: 0.4919 - accuracy: 0.74\n",
      "Epoch 00049: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.4939 - accuracy: 0.7399 - val_loss: 0.4993 - val_accuracy: 0.7371\n",
      "Epoch 50/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4968 - accuracy: 0.7376\n",
      "Epoch 00050: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4964 - accuracy: 0.7373 - val_loss: 0.4987 - val_accuracy: 0.7333\n",
      "Epoch 51/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.4933 - accuracy: 0.7421\n",
      "Epoch 00051: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4933 - accuracy: 0.7412 - val_loss: 0.4978 - val_accuracy: 0.7325\n",
      "Epoch 52/175\n",
      "19840/20080 [============================>.] - ETA: 0s - loss: 0.4919 - accuracy: 0.7416\n",
      "Epoch 00052: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4925 - accuracy: 0.7415 - val_loss: 0.4988 - val_accuracy: 0.7343\n",
      "Epoch 53/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.4885 - accuracy: 0.7443\n",
      "Epoch 00053: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4909 - accuracy: 0.7431 - val_loss: 0.4957 - val_accuracy: 0.7397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/175\n",
      "16896/20080 [========================>.....] - ETA: 0s - loss: 0.4914 - accuracy: 0.7438\n",
      "Epoch 00054: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4904 - accuracy: 0.7447 - val_loss: 0.4973 - val_accuracy: 0.7335\n",
      "Epoch 55/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.4908 - accuracy: 0.7427\n",
      "Epoch 00055: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4913 - accuracy: 0.7439 - val_loss: 0.4964 - val_accuracy: 0.7371\n",
      "Epoch 56/175\n",
      "16768/20080 [========================>.....] - ETA: 0s - loss: 0.4907 - accuracy: 0.7428\n",
      "Epoch 00056: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4920 - accuracy: 0.7429 - val_loss: 0.4969 - val_accuracy: 0.7349\n",
      "Epoch 57/175\n",
      "16000/20080 [======================>.......] - ETA: 0s - loss: 0.4911 - accuracy: 0.7419\n",
      "Epoch 00057: val_accuracy did not improve from 0.73969\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4888 - accuracy: 0.7428 - val_loss: 0.4992 - val_accuracy: 0.7373\n",
      "Epoch 58/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.4885 - accuracy: 0.7432\n",
      "Epoch 00058: val_accuracy improved from 0.73969 to 0.74168, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.4904 - accuracy: 0.7416 - val_loss: 0.4964 - val_accuracy: 0.7417\n",
      "Epoch 59/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.4919 - accuracy: 0.7387\n",
      "Epoch 00059: val_accuracy did not improve from 0.74168\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4909 - accuracy: 0.7399 - val_loss: 0.4983 - val_accuracy: 0.7383\n",
      "Epoch 60/175\n",
      "19328/20080 [===========================>..] - ETA: 0s - loss: 0.4926 - accuracy: 0.7430\n",
      "Epoch 00060: val_accuracy improved from 0.74168 to 0.74268, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 1s 26us/sample - loss: 0.4939 - accuracy: 0.7417 - val_loss: 0.4964 - val_accuracy: 0.7427\n",
      "Epoch 61/175\n",
      "15872/20080 [======================>.......] - ETA: 0s - loss: 0.4891 - accuracy: 0.7461\n",
      "Epoch 00061: val_accuracy improved from 0.74268 to 0.74308, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.4919 - accuracy: 0.7423 - val_loss: 0.4958 - val_accuracy: 0.7431\n",
      "Epoch 62/175\n",
      "18944/20080 [===========================>..] - ETA: 0s - loss: 0.4901 - accuracy: 0.7410\n",
      "Epoch 00062: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4892 - accuracy: 0.7420 - val_loss: 0.4977 - val_accuracy: 0.7357\n",
      "Epoch 63/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.4875 - accuracy: 0.7446\n",
      "Epoch 00063: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4868 - accuracy: 0.7438 - val_loss: 0.4950 - val_accuracy: 0.7387\n",
      "Epoch 64/175\n",
      "19840/20080 [============================>.] - ETA: 0s - loss: 0.4870 - accuracy: 0.7422\n",
      "Epoch 00064: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 18us/sample - loss: 0.4871 - accuracy: 0.7419 - val_loss: 0.4937 - val_accuracy: 0.7395\n",
      "Epoch 65/175\n",
      "18176/20080 [==========================>...] - ETA: 0s - loss: 0.4871 - accuracy: 0.7478\n",
      "Epoch 00065: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 25us/sample - loss: 0.4876 - accuracy: 0.7477 - val_loss: 0.4958 - val_accuracy: 0.7425\n",
      "Epoch 66/175\n",
      "19072/20080 [===========================>..] - ETA: 0s - loss: 0.4839 - accuracy: 0.7468\n",
      "Epoch 00066: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 23us/sample - loss: 0.4845 - accuracy: 0.7462 - val_loss: 0.4964 - val_accuracy: 0.7349\n",
      "Epoch 67/175\n",
      "18176/20080 [==========================>...] - ETA: 0s - loss: 0.4856 - accuracy: 0.7465\n",
      "Epoch 00067: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.4864 - accuracy: 0.7452 - val_loss: 0.4948 - val_accuracy: 0.7401\n",
      "Epoch 68/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.4882 - accuracy: 0.7429\n",
      "Epoch 00068: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.4888 - accuracy: 0.7420 - val_loss: 0.4971 - val_accuracy: 0.7353\n",
      "Epoch 69/175\n",
      "19968/20080 [============================>.] - ETA: 0s - loss: 0.4843 - accuracy: 0.7477\n",
      "Epoch 00069: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4843 - accuracy: 0.7480 - val_loss: 0.4940 - val_accuracy: 0.7397\n",
      "Epoch 70/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4868 - accuracy: 0.7451\n",
      "Epoch 00070: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4869 - accuracy: 0.7449 - val_loss: 0.4958 - val_accuracy: 0.7369\n",
      "Epoch 71/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.4873 - accuracy: 0.7465\n",
      "Epoch 00071: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4860 - accuracy: 0.7459 - val_loss: 0.4929 - val_accuracy: 0.7369\n",
      "Epoch 72/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.4878 - accuracy: 0.7473\n",
      "Epoch 00072: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4870 - accuracy: 0.7483 - val_loss: 0.4914 - val_accuracy: 0.7399\n",
      "Epoch 73/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.4859 - accuracy: 0.7478\n",
      "Epoch 00073: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4851 - accuracy: 0.7477 - val_loss: 0.4930 - val_accuracy: 0.7389\n",
      "Epoch 74/175\n",
      "16512/20080 [=======================>......] - ETA: 0s - loss: 0.4857 - accuracy: 0.7433\n",
      "Epoch 00074: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4831 - accuracy: 0.7460 - val_loss: 0.4942 - val_accuracy: 0.7375\n",
      "Epoch 75/175\n",
      "16768/20080 [========================>.....] - ETA: 0s - loss: 0.4858 - accuracy: 0.7425\n",
      "Epoch 00075: val_accuracy did not improve from 0.74308\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4853 - accuracy: 0.7438 - val_loss: 0.4955 - val_accuracy: 0.7325\n",
      "Epoch 76/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.4820 - accuracy: 0.7461\n",
      "Epoch 00076: val_accuracy improved from 0.74308 to 0.74447, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 1s 28us/sample - loss: 0.4841 - accuracy: 0.7454 - val_loss: 0.4946 - val_accuracy: 0.7445\n",
      "Epoch 77/175\n",
      "19712/20080 [============================>.] - ETA: 0s - loss: 0.4849 - accuracy: 0.7452\n",
      "Epoch 00077: val_accuracy did not improve from 0.74447\n",
      "20080/20080 [==============================] - 0s 18us/sample - loss: 0.4851 - accuracy: 0.7454 - val_loss: 0.4926 - val_accuracy: 0.7401\n",
      "Epoch 78/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.4845 - accuracy: 0.7467\n",
      "Epoch 00078: val_accuracy did not improve from 0.74447\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4821 - accuracy: 0.7498 - val_loss: 0.4929 - val_accuracy: 0.7375\n",
      "Epoch 79/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4825 - accuracy: 0.7477\n",
      "Epoch 00079: val_accuracy did not improve from 0.74447\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4828 - accuracy: 0.7471 - val_loss: 0.4929 - val_accuracy: 0.7397\n",
      "Epoch 80/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.4838 - accuracy: 0.7490\n",
      "Epoch 00080: val_accuracy improved from 0.74447 to 0.74587, saving model to data/best_model.h5\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4830 - accuracy: 0.7486 - val_loss: 0.4924 - val_accuracy: 0.7459\n",
      "Epoch 81/175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4820 - accuracy: 0.7505\n",
      "Epoch 00081: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4824 - accuracy: 0.7521 - val_loss: 0.4926 - val_accuracy: 0.7397\n",
      "Epoch 82/175\n",
      "18304/20080 [==========================>...] - ETA: 0s - loss: 0.4795 - accuracy: 0.7513\n",
      "Epoch 00082: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4806 - accuracy: 0.7511 - val_loss: 0.4926 - val_accuracy: 0.7393\n",
      "Epoch 83/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4779 - accuracy: 0.7521\n",
      "Epoch 00083: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4777 - accuracy: 0.7522 - val_loss: 0.4933 - val_accuracy: 0.7439\n",
      "Epoch 84/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4796 - accuracy: 0.7479\n",
      "Epoch 00084: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4793 - accuracy: 0.7482 - val_loss: 0.4936 - val_accuracy: 0.7383\n",
      "Epoch 85/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.4819 - accuracy: 0.7475\n",
      "Epoch 00085: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4814 - accuracy: 0.7470 - val_loss: 0.4959 - val_accuracy: 0.7347\n",
      "Epoch 86/175\n",
      "17664/20080 [=========================>....] - ETA: 0s - loss: 0.4791 - accuracy: 0.7492\n",
      "Epoch 00086: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4803 - accuracy: 0.7489 - val_loss: 0.4915 - val_accuracy: 0.7377\n",
      "Epoch 87/175\n",
      "18304/20080 [==========================>...] - ETA: 0s - loss: 0.4781 - accuracy: 0.7504\n",
      "Epoch 00087: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4788 - accuracy: 0.7508 - val_loss: 0.4943 - val_accuracy: 0.7431\n",
      "Epoch 88/175\n",
      "19456/20080 [============================>.] - ETA: 0s - loss: 0.4786 - accuracy: 0.7508\n",
      "Epoch 00088: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4790 - accuracy: 0.7507 - val_loss: 0.4907 - val_accuracy: 0.7433\n",
      "Epoch 89/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.4770 - accuracy: 0.7498\n",
      "Epoch 00089: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4790 - accuracy: 0.7481 - val_loss: 0.4924 - val_accuracy: 0.7419\n",
      "Epoch 90/175\n",
      "18176/20080 [==========================>...] - ETA: 0s - loss: 0.4767 - accuracy: 0.7572\n",
      "Epoch 00090: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4765 - accuracy: 0.7557 - val_loss: 0.4911 - val_accuracy: 0.7411\n",
      "Epoch 91/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4745 - accuracy: 0.7521\n",
      "Epoch 00091: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4766 - accuracy: 0.7498 - val_loss: 0.4931 - val_accuracy: 0.7443\n",
      "Epoch 92/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.4758 - accuracy: 0.7509\n",
      "Epoch 00092: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4747 - accuracy: 0.7519 - val_loss: 0.4914 - val_accuracy: 0.7401\n",
      "Epoch 93/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4783 - accuracy: 0.7502\n",
      "Epoch 00093: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4784 - accuracy: 0.7505 - val_loss: 0.4917 - val_accuracy: 0.7399\n",
      "Epoch 94/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4791 - accuracy: 0.7513\n",
      "Epoch 00094: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4779 - accuracy: 0.7520 - val_loss: 0.4920 - val_accuracy: 0.7435\n",
      "Epoch 95/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.4785 - accuracy: 0.7515\n",
      "Epoch 00095: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4788 - accuracy: 0.7521 - val_loss: 0.4906 - val_accuracy: 0.7437\n",
      "Epoch 96/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4781 - accuracy: 0.7523\n",
      "Epoch 00096: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4782 - accuracy: 0.7519 - val_loss: 0.4900 - val_accuracy: 0.7427\n",
      "Epoch 97/175\n",
      "16512/20080 [=======================>......] - ETA: 0s - loss: 0.4757 - accuracy: 0.7513\n",
      "Epoch 00097: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4756 - accuracy: 0.7524 - val_loss: 0.4920 - val_accuracy: 0.7409\n",
      "Epoch 98/175\n",
      "16640/20080 [=======================>......] - ETA: 0s - loss: 0.4722 - accuracy: 0.7575\n",
      "Epoch 00098: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4740 - accuracy: 0.7546 - val_loss: 0.4921 - val_accuracy: 0.7375\n",
      "Epoch 99/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.4732 - accuracy: 0.7553\n",
      "Epoch 00099: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4740 - accuracy: 0.7542 - val_loss: 0.4912 - val_accuracy: 0.7407\n",
      "Epoch 100/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4734 - accuracy: 0.7522\n",
      "Epoch 00100: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4742 - accuracy: 0.7520 - val_loss: 0.4934 - val_accuracy: 0.7441\n",
      "Epoch 101/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.4725 - accuracy: 0.7546\n",
      "Epoch 00101: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4744 - accuracy: 0.7534 - val_loss: 0.4927 - val_accuracy: 0.7377\n",
      "Epoch 102/175\n",
      "17664/20080 [=========================>....] - ETA: 0s - loss: 0.4758 - accuracy: 0.7529\n",
      "Epoch 00102: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4758 - accuracy: 0.7531 - val_loss: 0.4908 - val_accuracy: 0.7383\n",
      "Epoch 103/175\n",
      "17280/20080 [========================>.....] - ETA: 0s - loss: 0.4723 - accuracy: 0.7527\n",
      "Epoch 00103: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4725 - accuracy: 0.7521 - val_loss: 0.4950 - val_accuracy: 0.7373\n",
      "Epoch 104/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4759 - accuracy: 0.7501\n",
      "Epoch 00104: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4753 - accuracy: 0.7520 - val_loss: 0.4926 - val_accuracy: 0.7437\n",
      "Epoch 105/175\n",
      "17792/20080 [=========================>....] - ETA: 0s - loss: 0.4722 - accuracy: 0.7528\n",
      "Epoch 00105: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4725 - accuracy: 0.7523 - val_loss: 0.4927 - val_accuracy: 0.7403\n",
      "Epoch 106/175\n",
      "16896/20080 [========================>.....] - ETA: 0s - loss: 0.4695 - accuracy: 0.7560\n",
      "Epoch 00106: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4701 - accuracy: 0.7556 - val_loss: 0.4913 - val_accuracy: 0.7369\n",
      "Epoch 107/175\n",
      "17024/20080 [========================>.....] - ETA: 0s - loss: 0.4759 - accuracy: 0.7543\n",
      "Epoch 00107: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4748 - accuracy: 0.7528 - val_loss: 0.4906 - val_accuracy: 0.7341\n",
      "Epoch 108/175\n",
      "17664/20080 [=========================>....] - ETA: 0s - loss: 0.4694 - accuracy: 0.7590\n",
      "Epoch 00108: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4725 - accuracy: 0.7556 - val_loss: 0.4934 - val_accuracy: 0.7351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/175\n",
      "19584/20080 [============================>.] - ETA: 0s - loss: 0.4732 - accuracy: 0.7520\n",
      "Epoch 00109: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.4731 - accuracy: 0.7517 - val_loss: 0.4938 - val_accuracy: 0.7379\n",
      "Epoch 110/175\n",
      "16640/20080 [=======================>......] - ETA: 0s - loss: 0.4732 - accuracy: 0.7554\n",
      "Epoch 00110: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4735 - accuracy: 0.7552 - val_loss: 0.4923 - val_accuracy: 0.7403\n",
      "Epoch 111/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4724 - accuracy: 0.7509\n",
      "Epoch 00111: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4710 - accuracy: 0.7535 - val_loss: 0.4951 - val_accuracy: 0.7331\n",
      "Epoch 112/175\n",
      "19968/20080 [============================>.] - ETA: 0s - loss: 0.4717 - accuracy: 0.7533\n",
      "Epoch 00112: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4715 - accuracy: 0.7534 - val_loss: 0.4924 - val_accuracy: 0.7359\n",
      "Epoch 113/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.4712 - accuracy: 0.7575\n",
      "Epoch 00113: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 18us/sample - loss: 0.4717 - accuracy: 0.7556 - val_loss: 0.4943 - val_accuracy: 0.7339\n",
      "Epoch 114/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4685 - accuracy: 0.7547\n",
      "Epoch 00114: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.4710 - accuracy: 0.7542 - val_loss: 0.4912 - val_accuracy: 0.7411\n",
      "Epoch 115/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4673 - accuracy: 0.7596\n",
      "Epoch 00115: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4686 - accuracy: 0.7584 - val_loss: 0.4934 - val_accuracy: 0.7389\n",
      "Epoch 116/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4704 - accuracy: 0.7566\n",
      "Epoch 00116: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 19us/sample - loss: 0.4733 - accuracy: 0.7536 - val_loss: 0.4926 - val_accuracy: 0.7351\n",
      "Epoch 117/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4732 - accuracy: 0.7522\n",
      "Epoch 00117: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4712 - accuracy: 0.7541 - val_loss: 0.4903 - val_accuracy: 0.7399\n",
      "Epoch 118/175\n",
      "19072/20080 [===========================>..] - ETA: 0s - loss: 0.4659 - accuracy: 0.7600\n",
      "Epoch 00118: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 21us/sample - loss: 0.4667 - accuracy: 0.7594 - val_loss: 0.4942 - val_accuracy: 0.7381\n",
      "Epoch 119/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.4692 - accuracy: 0.7571\n",
      "Epoch 00119: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4694 - accuracy: 0.7574 - val_loss: 0.4945 - val_accuracy: 0.7355\n",
      "Epoch 120/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4707 - accuracy: 0.7574\n",
      "Epoch 00120: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4713 - accuracy: 0.7563 - val_loss: 0.4912 - val_accuracy: 0.7457\n",
      "Epoch 121/175\n",
      "19456/20080 [============================>.] - ETA: 0s - loss: 0.4702 - accuracy: 0.7562\n",
      "Epoch 00121: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 18us/sample - loss: 0.4700 - accuracy: 0.7565 - val_loss: 0.4937 - val_accuracy: 0.7421\n",
      "Epoch 122/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.4712 - accuracy: 0.7528\n",
      "Epoch 00122: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4715 - accuracy: 0.7526 - val_loss: 0.4933 - val_accuracy: 0.7375\n",
      "Epoch 123/175\n",
      "16768/20080 [========================>.....] - ETA: 0s - loss: 0.4700 - accuracy: 0.7574\n",
      "Epoch 00123: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 20us/sample - loss: 0.4717 - accuracy: 0.7562 - val_loss: 0.4961 - val_accuracy: 0.7361\n",
      "Epoch 124/175\n",
      "17664/20080 [=========================>....] - ETA: 0s - loss: 0.4686 - accuracy: 0.7578\n",
      "Epoch 00124: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4685 - accuracy: 0.7578 - val_loss: 0.4936 - val_accuracy: 0.7435\n",
      "Epoch 125/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4694 - accuracy: 0.7599\n",
      "Epoch 00125: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4699 - accuracy: 0.7588 - val_loss: 0.4944 - val_accuracy: 0.7433\n",
      "Epoch 126/175\n",
      "19072/20080 [===========================>..] - ETA: 0s - loss: 0.4667 - accuracy: 0.7602\n",
      "Epoch 00126: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.4668 - accuracy: 0.7602 - val_loss: 0.4941 - val_accuracy: 0.7417\n",
      "Epoch 127/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4716 - accuracy: 0.7576\n",
      "Epoch 00127: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 17us/sample - loss: 0.4714 - accuracy: 0.7580 - val_loss: 0.4975 - val_accuracy: 0.7401\n",
      "Epoch 128/175\n",
      "19584/20080 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7540\n",
      "Epoch 00128: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4691 - accuracy: 0.7540 - val_loss: 0.4949 - val_accuracy: 0.7425\n",
      "Epoch 129/175\n",
      "16384/20080 [=======================>......] - ETA: 0s - loss: 0.4664 - accuracy: 0.7591\n",
      "Epoch 00129: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 15us/sample - loss: 0.4686 - accuracy: 0.7576 - val_loss: 0.4956 - val_accuracy: 0.7369\n",
      "Epoch 130/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4700 - accuracy: 0.7600\n",
      "Epoch 00130: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4690 - accuracy: 0.7597 - val_loss: 0.4937 - val_accuracy: 0.7431\n",
      "Epoch 131/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4674 - accuracy: 0.7630\n",
      "Epoch 00131: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4668 - accuracy: 0.7632 - val_loss: 0.4943 - val_accuracy: 0.7439\n",
      "Epoch 132/175\n",
      "19712/20080 [============================>.] - ETA: 0s - loss: 0.4684 - accuracy: 0.7556\n",
      "Epoch 00132: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 25us/sample - loss: 0.4688 - accuracy: 0.7554 - val_loss: 0.4960 - val_accuracy: 0.7401\n",
      "Epoch 133/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.4667 - accuracy: 0.7596\n",
      "Epoch 00133: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4668 - accuracy: 0.7591 - val_loss: 0.4948 - val_accuracy: 0.7421\n",
      "Epoch 134/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4688 - accuracy: 0.7585\n",
      "Epoch 00134: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4684 - accuracy: 0.7584 - val_loss: 0.4949 - val_accuracy: 0.7395\n",
      "Epoch 135/175\n",
      "19456/20080 [============================>.] - ETA: 0s - loss: 0.4661 - accuracy: 0.7604\n",
      "Epoch 00135: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 16us/sample - loss: 0.4649 - accuracy: 0.7622 - val_loss: 0.4959 - val_accuracy: 0.7345\n",
      "Epoch 136/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4640 - accuracy: 0.7610\n",
      "Epoch 00136: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4654 - accuracy: 0.7619 - val_loss: 0.4942 - val_accuracy: 0.7413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/175\n",
      "17152/20080 [========================>.....] - ETA: 0s - loss: 0.4683 - accuracy: 0.7583\n",
      "Epoch 00137: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4682 - accuracy: 0.7598 - val_loss: 0.4957 - val_accuracy: 0.7359\n",
      "Epoch 138/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4662 - accuracy: 0.7593\n",
      "Epoch 00138: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4664 - accuracy: 0.7586 - val_loss: 0.4953 - val_accuracy: 0.7381\n",
      "Epoch 139/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4717 - accuracy: 0.7564\n",
      "Epoch 00139: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4696 - accuracy: 0.7575 - val_loss: 0.4939 - val_accuracy: 0.7423\n",
      "Epoch 140/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4652 - accuracy: 0.7599\n",
      "Epoch 00140: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4649 - accuracy: 0.7611 - val_loss: 0.4948 - val_accuracy: 0.7387\n",
      "Epoch 141/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4678 - accuracy: 0.7570\n",
      "Epoch 00141: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4681 - accuracy: 0.7562 - val_loss: 0.4947 - val_accuracy: 0.7363\n",
      "Epoch 142/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4713 - accuracy: 0.7617\n",
      "Epoch 00142: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4694 - accuracy: 0.7620 - val_loss: 0.4936 - val_accuracy: 0.7433\n",
      "Epoch 143/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4670 - accuracy: 0.7590\n",
      "Epoch 00143: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4686 - accuracy: 0.7581 - val_loss: 0.4933 - val_accuracy: 0.7411\n",
      "Epoch 144/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4633 - accuracy: 0.7617\n",
      "Epoch 00144: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4653 - accuracy: 0.7609 - val_loss: 0.4948 - val_accuracy: 0.7417\n",
      "Epoch 145/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4657 - accuracy: 0.7544\n",
      "Epoch 00145: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4653 - accuracy: 0.7550 - val_loss: 0.4933 - val_accuracy: 0.7423\n",
      "Epoch 146/175\n",
      "17664/20080 [=========================>....] - ETA: 0s - loss: 0.4670 - accuracy: 0.7587\n",
      "Epoch 00146: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4655 - accuracy: 0.7605 - val_loss: 0.4946 - val_accuracy: 0.7397\n",
      "Epoch 147/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4646 - accuracy: 0.7603\n",
      "Epoch 00147: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4650 - accuracy: 0.7615 - val_loss: 0.4904 - val_accuracy: 0.7429\n",
      "Epoch 148/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4613 - accuracy: 0.7632\n",
      "Epoch 00148: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4636 - accuracy: 0.7612 - val_loss: 0.4920 - val_accuracy: 0.7443\n",
      "Epoch 149/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4618 - accuracy: 0.7661\n",
      "Epoch 00149: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4636 - accuracy: 0.7648 - val_loss: 0.4940 - val_accuracy: 0.7429\n",
      "Epoch 150/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4653 - accuracy: 0.7599\n",
      "Epoch 00150: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4647 - accuracy: 0.7607 - val_loss: 0.4945 - val_accuracy: 0.7435\n",
      "Epoch 151/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4675 - accuracy: 0.7587\n",
      "Epoch 00151: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4686 - accuracy: 0.7583 - val_loss: 0.4956 - val_accuracy: 0.7377\n",
      "Epoch 152/175\n",
      "17792/20080 [=========================>....] - ETA: 0s - loss: 0.4632 - accuracy: 0.7612\n",
      "Epoch 00152: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4630 - accuracy: 0.7613 - val_loss: 0.4964 - val_accuracy: 0.7397\n",
      "Epoch 153/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4626 - accuracy: 0.7660\n",
      "Epoch 00153: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4639 - accuracy: 0.7651 - val_loss: 0.4925 - val_accuracy: 0.7419\n",
      "Epoch 154/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4620 - accuracy: 0.7609\n",
      "Epoch 00154: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4626 - accuracy: 0.7602 - val_loss: 0.4938 - val_accuracy: 0.7439\n",
      "Epoch 155/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4648 - accuracy: 0.7652\n",
      "Epoch 00155: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4640 - accuracy: 0.7647 - val_loss: 0.4931 - val_accuracy: 0.7405\n",
      "Epoch 156/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4641 - accuracy: 0.7629\n",
      "Epoch 00156: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4646 - accuracy: 0.7628 - val_loss: 0.4941 - val_accuracy: 0.7415\n",
      "Epoch 157/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4658 - accuracy: 0.7586\n",
      "Epoch 00157: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4643 - accuracy: 0.7599 - val_loss: 0.4945 - val_accuracy: 0.7419\n",
      "Epoch 158/175\n",
      "17664/20080 [=========================>....] - ETA: 0s - loss: 0.4644 - accuracy: 0.7574\n",
      "Epoch 00158: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4641 - accuracy: 0.7571 - val_loss: 0.4970 - val_accuracy: 0.7399\n",
      "Epoch 159/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4640 - accuracy: 0.7599\n",
      "Epoch 00159: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4626 - accuracy: 0.7603 - val_loss: 0.4958 - val_accuracy: 0.7395\n",
      "Epoch 160/175\n",
      "17792/20080 [=========================>....] - ETA: 0s - loss: 0.4636 - accuracy: 0.7637\n",
      "Epoch 00160: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4655 - accuracy: 0.7635 - val_loss: 0.4938 - val_accuracy: 0.7405\n",
      "Epoch 161/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4637 - accuracy: 0.7632\n",
      "Epoch 00161: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4638 - accuracy: 0.7623 - val_loss: 0.4907 - val_accuracy: 0.7445\n",
      "Epoch 162/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4663 - accuracy: 0.7599\n",
      "Epoch 00162: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4668 - accuracy: 0.7594 - val_loss: 0.4965 - val_accuracy: 0.7401\n",
      "Epoch 163/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4640 - accuracy: 0.7647\n",
      "Epoch 00163: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4644 - accuracy: 0.7634 - val_loss: 0.4925 - val_accuracy: 0.7389\n",
      "Epoch 164/175\n",
      "18176/20080 [==========================>...] - ETA: 0s - loss: 0.4600 - accuracy: 0.7643\n",
      "Epoch 00164: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4608 - accuracy: 0.7642 - val_loss: 0.4960 - val_accuracy: 0.7429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4630 - accuracy: 0.7634\n",
      "Epoch 00165: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4619 - accuracy: 0.7629 - val_loss: 0.4964 - val_accuracy: 0.7375\n",
      "Epoch 166/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4632 - accuracy: 0.7600\n",
      "Epoch 00166: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4630 - accuracy: 0.7599 - val_loss: 0.4961 - val_accuracy: 0.7359\n",
      "Epoch 167/175\n",
      "18304/20080 [==========================>...] - ETA: 0s - loss: 0.4630 - accuracy: 0.7619\n",
      "Epoch 00167: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4623 - accuracy: 0.7617 - val_loss: 0.4999 - val_accuracy: 0.7337\n",
      "Epoch 168/175\n",
      "18176/20080 [==========================>...] - ETA: 0s - loss: 0.4634 - accuracy: 0.7591\n",
      "Epoch 00168: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4632 - accuracy: 0.7606 - val_loss: 0.4986 - val_accuracy: 0.7409\n",
      "Epoch 169/175\n",
      "17408/20080 [=========================>....] - ETA: 0s - loss: 0.4594 - accuracy: 0.7621\n",
      "Epoch 00169: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4621 - accuracy: 0.7619 - val_loss: 0.4959 - val_accuracy: 0.7413\n",
      "Epoch 170/175\n",
      "17792/20080 [=========================>....] - ETA: 0s - loss: 0.4613 - accuracy: 0.7619\n",
      "Epoch 00170: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4620 - accuracy: 0.7611 - val_loss: 0.4933 - val_accuracy: 0.7417\n",
      "Epoch 171/175\n",
      "17536/20080 [=========================>....] - ETA: 0s - loss: 0.4588 - accuracy: 0.7629\n",
      "Epoch 00171: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 14us/sample - loss: 0.4584 - accuracy: 0.7626 - val_loss: 0.4985 - val_accuracy: 0.7397\n",
      "Epoch 172/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4586 - accuracy: 0.7644\n",
      "Epoch 00172: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4618 - accuracy: 0.7625 - val_loss: 0.4962 - val_accuracy: 0.7419\n",
      "Epoch 173/175\n",
      "17920/20080 [=========================>....] - ETA: 0s - loss: 0.4621 - accuracy: 0.7658\n",
      "Epoch 00173: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4616 - accuracy: 0.7644 - val_loss: 0.4954 - val_accuracy: 0.7377\n",
      "Epoch 174/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4612 - accuracy: 0.7611\n",
      "Epoch 00174: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4610 - accuracy: 0.7619 - val_loss: 0.4945 - val_accuracy: 0.7439\n",
      "Epoch 175/175\n",
      "18048/20080 [=========================>....] - ETA: 0s - loss: 0.4595 - accuracy: 0.7675\n",
      "Epoch 00175: val_accuracy did not improve from 0.74587\n",
      "20080/20080 [==============================] - 0s 13us/sample - loss: 0.4599 - accuracy: 0.7663 - val_loss: 0.4945 - val_accuracy: 0.7409\n"
     ]
    }
   ],
   "source": [
    "hist = m.fit( Z_train,\n",
    "                    y_train, \n",
    "                    batch_size=128,\n",
    "                    validation_data=(Z_test, y_test),\n",
    "                    epochs=175,\n",
    "                    callbacks=[mc],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(hist.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.plot(hist.history['accuracy'], label='Train Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_one_name='Cilic M.'\n",
    "player_two_name='Raonic M.'\n",
    "Surface='Hard'\n",
    "Major=1\n",
    "best_of=5\n",
    "vals=[Surface,Major,best_of]\n",
    "cols_to_get_one=['player_one_rank','one_win_rate_year',\n",
    "       'one_games_played_year', 'one_clay_year', 'one_grass_year',\n",
    "       'one_hard_year', 'one_three_year', 'one_five_year','one_hard_vv','one_win_rate_vv','one_games_played_vv'\n",
    "                ,'one_three_vv','one_five_vv','player_one_pts','one_clay_vv','one_grass_vv']\n",
    "\n",
    "cols_to_get_two=['player_two_rank','two_win_rate_year',\n",
    "       'two_games_played_year', 'two_clay_year', 'two_grass_year',\n",
    "       'two_hard_year', 'two_three_year', 'two_five_year','two_hard_vv','two_win_rate_vv',\n",
    "                 'two_games_played_vv','two_three_vv','two_five_vv','player_two_pts','two_clay_vv',\n",
    "                 'two_grass_vv']\n",
    "\n",
    "one=list(data[data.player_one==player_one_name][cols_to_get_one].head(1).values[0])\n",
    "two=list(data[data.player_two==player_two_name][cols_to_get_two].head(1).values[0])\n",
    "\n",
    "temp=vals+one+two\n",
    "temp=pd.DataFrame(temp).T\n",
    "temp.columns=['Surface','major', 'Best of','player_one_rank','one_win_rate_year',\n",
    "       'one_games_played_year', 'one_clay_year', 'one_grass_year',\n",
    "       'one_hard_year', 'one_three_year', 'one_five_year', 'one_hard_vv','one_win_rate_vv','one_games_played_vv','one_three_vv',\n",
    "              'one_five_vv','player_one_pts','one_clay_vv','one_grass_vv'\n",
    "        ,'player_two_rank','two_win_rate_year',\n",
    "       'two_games_played_year', 'two_clay_year', 'two_grass_year',\n",
    "       'two_hard_year', 'two_three_year', 'two_five_year','two_hard_vv','two_win_rate_vv','two_games_played_vv','two_three_vv',\n",
    "              'two_five_vv','player_two_pts','two_clay_vv','two_grass_vv'\n",
    "             ]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['pts_dif']=temp['player_two_pts']-temp['player_one_pts']\n",
    "temp['rank_dif']=temp['player_two_rank']-temp['player_one_rank']\n",
    "temp['win_rate_diff']=temp['two_win_rate_year']-temp['one_win_rate_year']\n",
    "temp['hard_diff']=temp['two_hard_year']-temp['one_hard_year']\n",
    "temp['five_diff']=temp['two_five_year']-temp['one_five_year']\n",
    "temp['three_diff']=temp['two_three_year']-temp['one_three_year']\n",
    "\n",
    "temp['hard_vv']=temp['two_hard_vv']-temp['one_hard_vv']\n",
    "temp['vv']=temp['two_win_rate_vv']-temp['one_win_rate_vv']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Surface', 'major', 'Best of', 'player_one_rank', 'one_win_rate_year',\n",
       "       'one_games_played_year', 'one_clay_year', 'one_grass_year',\n",
       "       'one_hard_year', 'one_three_year', 'one_five_year', 'one_hard_vv',\n",
       "       'one_win_rate_vv', 'one_games_played_vv', 'one_three_vv', 'one_five_vv',\n",
       "       'player_one_pts', 'one_clay_vv', 'one_grass_vv', 'player_two_rank',\n",
       "       'two_win_rate_year', 'two_games_played_year', 'two_clay_year',\n",
       "       'two_grass_year', 'two_hard_year', 'two_three_year', 'two_five_year',\n",
       "       'two_hard_vv', 'two_win_rate_vv', 'two_games_played_vv', 'two_three_vv',\n",
       "       'two_five_vv', 'player_two_pts', 'two_clay_vv', 'two_grass_vv',\n",
       "       'pts_dif', 'rank_dif', 'win_rate_diff', 'hard_diff', 'five_diff',\n",
       "       'three_diff', 'hard_vv', 'vv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_one_rank</th>\n",
       "      <th>player_two_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  player_one_rank player_two_rank\n",
       "0               3              15"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[['player_one_rank', 'player_two_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_temp=mapper.transform(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8251812]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(Z_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
